---
title: "Introducción a las cadenas de Markov"
author: "Ricardo Alberich, Juan Gabriel Gomila y Arnau Mir"
date: ""
output:
  beamer_presentation:
#    colortheme: rose
    incremental: yes
    theme: Warsaw
    toc: no

header-includes: \usepackage{amsmath,color,array,booktabs,algorithm2e}
                 \newcommand\blue[1]{\textcolor{blue}{#1}}
                 \newcommand\red[1]{\textcolor{red}{#1}}

                 
              
---



## Introducción a los procesos estocásticos

* El mundo que nos rodea es dinámico, va cambiando con el tiempo:
   * la temperatura,
   * los precios de las acciones,
   * popularidad de los políticos,
   * el uso de la CPU de un ordenador,
   * la velocidad de una determinada conexión a internet,
   * etc.

* De cara a modelar una determinada situación, necesitamos que las \red{variables aleatorias} cambien con el \red{tiempo.}

* Por dicho motivo, vamos a introducir los \red{procesos estocásticos}, que son \blue{variables aleatorias} que además de depender de los elementos del \blue{espacio muestral}, dependen del \red{tiempo.}

## Procesos estocásticos
\begin{block}{Procesos estocásticos}
Un \red{proceso estocástico} es una \red{variable aleatoria} $X(w,t)$ que depende de dos argumentos: 
\begin{itemize}
\item un elemento del \blue{espacio muestral} $w\in\Omega$, 
\item el \red{tiempo} $t\in {\cal T}$, donde el conjunto ${\cal T}$ puede ser 
\begin{itemize}
\item discreto: ${\cal T}=\{0,1,2,\ldots\}$, o  ${\cal T}=\{\ldots,-2,-1,0,1,2,\ldots\}$,
\item continuo: ${\cal T}=[0,\infty)$ o ${\cal T}=[-\infty,\infty)$.
\end{itemize}
\end{itemize}
\end{block}

* Si fijamos el \red{tiempo} $t$, la \blue{variable aleatoria} $X(w,t)=X_t(w)$ sería una \blue{variable aleatoria} que modelizaría lo que pasa en el sistema en el \red{instante} $t$.

* Si fijamos el elemento $w$ del espacio muestral $\Omega$, tenemos la función dependiendo del \red{tiempo} $X(w,t)=X_w(t)$. Dicha función se denomina trayectoria del \blue{proceso estocástico.}

## Procesos estocásticos
\begin{block}{Ejemplo: lanzamiento de una moneda}
Un \blue{proceso estocástico} sencillo es considerar lanzar una moneda cada cierto espacio de tiempo, por ejemplo cada minuto, y observar el resultado. 

En este caso $\Omega =\{c,+\}$ y ${\cal T}=\{0,1,2,\ldots\}$.

$X(w,t)$ sería la variable aleatoria que nos dice el comportamiento de la moneda en el lanzamiento $t$-ésimo. 

La distribución de dicha variable será de Bernoulli de parámetro $p$ para cualquier instante $t$ donde $p$ es la probabilidad de sacar cara.
\end{block}

## Procesos estocásticos
```{r,echo=FALSE}
set.seed=2020
resultats=rbinom(20,1,0.7)
plot(1:length(resultats),resultats, xaxt="n",yaxt="n",type="p",pch=20,cex=0.5,xlab="t",ylab="Resultados",ylim = c(0,1),col="red")
xtick=1:length(resultats)
axis(side=1, at=xtick, labels=TRUE)
ytick=c(0,1)
axis(side=2, at=ytick, labels = TRUE)
```


## Procesos estocásticos
\begin{block}{Tipos de procesos estocásticos}
Diremos que el \blue{proceso estocástico} es \red{de estado discreto} si la variable aleatoria $X_t(w)$ es discreta y es \red{de estado continuo} si la variable aleatoria $X_t(w)$ es continua para todo valor del tiempo $t$.

Diremos que el \blue{proceso estocástico} es \red{de tiempo discreto} si el conjunto ${\cal T}$ de valores del tiempo es discreto y es \red{de tiempo continuo} si ${\cal T}
$ es continuo.
\end{block}

## Procesos estocásticos. Ejemplos

* Ejemplo anterior del lanzamiento de una moneda de parámetro $p$: estado discreto y tiempo discreto.
* Número de infectados diarios por una pandemia: estado discreto y tiempo discreto.
* Temperatura en un lugar determinado: estado continuo y tiempo continuo.
* Tiempo de espera del $n$-ésimo cliente de una cola en el supermercado modelado como $X(n,w)$. Este ejemplo es "lioso" ya que el tiempo sería el número del cliente y el espacio muestral sería precisamente el tiempo. Pensar que la variable aleatoria "tiempo" dependerá del número de cliente $n$: estado continuo y tiempo discreto. 

## Procesos de Markov

\begin{block}{Cadena de Markov}
Un proceso estocástico $X(t)$ es un \red{proceso de Markov} si para cualquier secuencia de valores $t_1<\cdots < t_n <t$ y para cualquier secuencia de sucesos $A,A_1,\ldots,A_n$,
\begin{align*}
& P(X(t)\in A| X(t_1)\in A_1,\ldots,X(t_n)\in A_n)\\ & =P(X(t)\in A|X(t_n)\in A_n).
\end{align*}
\end{block}

* Es decir, que la \red{distribución condicionada} de la variable $X(t)$ condicionada a los valores del proceso estocástico en $n$ \blue{instantes cualesquiera del pasado} sólo depende de la \red{distribución condicionada} de la variable $X(t)$ condicionada al proceso correspondiente al \red{último instante de la secuencia.}

## Procesos de Markov
* Esquemáticamente, podemos escribir:
\begin{align*}
& P(\mathrm{FUTURO}|\mathrm{PASADO,\ PRESENTE})\\ & =P(\mathrm{FUTURO}|\mathrm{PRESENTE}).
\end{align*}

* Ejemplos:
  * Temperatura en un día determinado: no es un \red{proceso de Markov}.
  * Número de conexiones registradas en un router de internet en un instante determinado: sí es un \red{proceso de Markov} ya que la gente se conecta aleatoriamente.
  * Precio de un stock en la bolsa en un día determinado: no es un \red{proceso de Markov}.
  * Ejemplo del lanzamiento de la moneda: sí es un \red{proceso de Markov}.
  
## Cadenas de Markov. Introducción
* Dentro de los \red{procesos de Markov} están las denominadas \red{cadenas de Markov}:

\begin{block}{Cadenas de Markov}
Una \red{cadena de Markov} es un \blue{proceso de Markov} de \blue{estado discreto} y de \blue{tiempo discreto.}
\end{block}

* Las cadenas de Markov se aplican a:
  * Meteorología: modelización de modelos meteorológicos básicos.
  * Modelos epidemiológicos: modelización de una epidemia.
  * Internet: el pagerank usado por Google para dar un peso a las páginas web.
  * Economía y finanzas: modelización del colapso de una bolsa de valores.
  * Genética: teoría genética de poblaciones.
  * Redes neuronales: se utilizan en las máquinas de Boltzmann.
  
  
## Cadenas de Markov. Introducción

* Vamos a introducir algunas simplificaciones:
  * Definiremos ${\cal T}=\{0,1,2\ldots\}$. Entonces la \red{cadena de Markov} sería una \blue{secuencia aleatoria} de variables aleatorias $X(0),X(1),X(2),\ldots$.
  * Llamaremos al conjunto $\Omega$ \red{conjunto de estados.} Como es discreto, lo enumeraremos de la forma siguiente $\Omega =\{1,2,\ldots,n\}$.
  
## Cadenas de Markov. Introducción
* Por ser una \red{cadena de Markov}, un \blue{proceso de Markov}, tenemos la denominada \red{propiedad de Markov} que nos dice que la variable aleatoria en un instante $t+1$ sólo depende de los valores que toma la variable aleatoria $X(t)$ o el proceso en el instante $t$.

* Es decir, $p_{ij}(t)=P(X(t+1)=j|X(t)=i)$ vale:
\begin{align*}
p_{ij}(t)= & P(X(t+1)=j|X(t)=i)\\ = & P(X(t+1)=j|X(t)=i,X(t-1)=h,X(t-2)=g,\ldots)
\end{align*}

## Cadenas de Markov. Probabilidades de transición
\begin{block}{Probabilidades de transición}
Las probabilidades $p_{ij}(t)$ se llaman \red{probabilidades de transición}.

La probabilidad
$$
p_{ij}^{(h)}(t)=P(X(t+h)=j|X(t)=i),
$$
es la probabilidad de ir desde el \blue{estado} $i$ hasta el \blue{estado} $j$ usando $h$ \blue{transiciones}. Dicha probabilidad se llama \red{probabilidad de transición de $h$ pasos.}
\end{block}

## Cadenas de Markov. Cadenas de Markov homogéneas
\begin{block}{Cadenas de Markov homogéneas}
Una \blue{cadena de Markov} es \red{homogénea} cuando las \red{probabilidades de transición} no dependen del tiempo $t$. 

Es decir:
$$
p_{ij}(t)=p_{ij},\quad p_{ij}^{(h)}=p_{ij}^{(h)}.
$$
\end{block}

* Notación:
  * $p_{ij}=P(X(t+1)=j|X(t)=i)$: probabilidad de transición.
  * $p_{ij}^{(h)}=P(X(t+h)=j|X(t)=i)$: probabilidad de transición en $h$ pasos.
  * $P_t(x)=P(X(t)=x)$: distribución de la variable aleatoria $X(t)$.
  * $P_0(t)=P(X(0)=x)$: distribución inicial o en el tiempo $0$.
  
  
## Cadenas de Markov. Ejemplo
\begin{block}{Ejemplo}
Un ordenador es compartido por dos usuarios. Estudiamos \blue{cuantos usuarios hay conectados cada minuto.} Nos dicen que un usuario puede \red{desconectarse con probabilidad $0.3$} y se puede \red{conectar con probabilidad $0.4$.}

Modelizamos la situación anterior con una \blue{cadena de Markov $X(t)$} que nos da el \blue{número de usuarios conectados al cabo de $t$ minutos.}

Los valores de $X(t)$ pueden ser $0,1$ y $2$. Por tanto, el conjunto de estados será: $\Omega =\{0,1,2\}$.

A continuación, calculemos las \red{probabilidad de transición:}
\end{block}

## Cadenas de Markov. Ejemplo
\begin{block}{Ejemplo}
Si $X(0)=0$, o no hay ningún usuario conectado, el número de usuarios que habrá en el siguiente minuto es una variable aleatoria \red{binomial} de parámetros $n=2$ y $p=0.4$:
\begin{align*}
p_{00}= & \binom{2}{0}\cdot 0.4^0\cdot 0.6^2 =`r 0.6^2`,\\
p_{01}= & \binom{2}{1}\cdot 0.4^1\cdot 0.6^1 =`r 2*0.6*0.4`,\\
p_{02}=& \binom{2}{2}\cdot 0.4^2\cdot 0.6^0 =`r 0.4^2`.
\end{align*}
\end{block}

## Cadenas de Markov. Ejemplo
\begin{block}{Ejemplo}
Si $X(0)=1$, o hay un usuario conectado, el número de usuarios nuevos que habrá en el siguiente minuto es una variable aleatoria \red{binomial} de parámetros $n=1$ y $p=0.4$ y el número de desconexiones en el minuto siguiente será una variable aleatoria \red{binomial} de parámetros $n=1$ y $p=0.3$:
\begin{align*}
p_{10}= & 0.6\cdot 0.3  =`r 0.6*0.3`,\\
p_{11}= & 0.3\cdot 0.4 + 0.7\cdot 0.6 =`r 0.3*0.4+0.7*0.6`,\\
p_{12}=& 0.7\cdot 0.4 =`r 0.7*0.4`.
\end{align*}
\end{block}

## Cadenas de Markov. Ejemplo
\begin{block}{Ejemplo}
Si $X(0)=2$, o hay los dos usuarios conectados, el número de usuarios desconectados que habrá en el siguiente minuto es una variable aleatoria \red{binomial} de parámetros $n=2$ y $p=0.3$. 
\begin{align*}
p_{20}= & \binom{2}{0}\cdot 0.7^0\cdot 0.3^2 =`r 0.3^2`,\\
p_{21}= & \binom{2}{1}\cdot 0.7^1\cdot 0.3^1=`r 2*0.7*0.3`,\\
p_{22}=& \binom{2}{2}\cdot 0.7^2\cdot 0.3^0 =`r 0.7^2`.
\end{align*}
\end{block}

## Cadenas de Markov. Diagrama de transición
Diagrama de transición del ejemplo.
\begin{center}
\includegraphics{markov.png}
\end{center}

## Cadenas de Markov. Ejemplo en R
\begin{block}{Ejemplo en R}

Las probabilidades anteriores se almacenarían en R de la forma siguiente.

Vemos que la suma de las filas vale 1:
\end{block}
```{r}
P=matrix(c(0.36,0.48,0.16,0.18,0.54,0.28,0.09,0.42,0.49),
         3,3,byrow=T)
P
apply(P,1,sum)
```

## Cadenas de Markov. Ejemplo en python
\begin{block}{Ejemplo en python}

\end{block}
```{python}
import numpy as np
P=np.matrix([[0.36,0.48,0.16],[0.18,0.54,0.28],
[0.09,0.42,0.49]]) 
P
np.sum(P, axis = 1)
```

## Cadenas de Markov. Matriz de transición
* Las \red{probabilidades de transición} se pueden escribir en forma de matriz de la forma siguiente:
$$
\mathbf{P}=\begin{pmatrix}
p_{11}&p_{12}&\cdots &p_{1n}\\
p_{21}&p_{22}&\cdots &p_{2n}\\
\vdots &\vdots &\vdots &\vdots\\
p_{n1}&p_{n2}&\cdots &p_{nn}
\end{pmatrix},
$$
donde $\mathbf{P}$ se llama \red{matriz de probabilidades de transición.}

* El valor $p_{ij}$ es la \red{probabilidad de transición} desde el \blue{estado} $i$ al \blue{estado} $j$.

## Cadenas de Markov. Matriz de transición
* La \red{matriz de probabilidades de transición} es una matriz \red{estocástica}, es decir la suma de las filas vale $1$:
$$
p_{i1}+p_{i2}+\cdots + p_{in}=1.
$$

* Recordemos que $p_{ij}=P(X(1)=j|X(0)=i)$. Por tanto el vector $(p_{i1},\ldots, p_{in})$ representaría la \red{función de masa de probabilidad} de la variable aleatoria discreta $X(1)|X(0)=i$.

## Cadenas de Markov. Ejemplo
\begin{block}{Ejemplo anterior}
La \red{matriz de probabilidades de transición} sería la siguiente:
```{r,echo=FALSE}
#library(xtable)
P=matrix(c(0.36,0.48,0.16,0.18,0.54,0.28,0.09,0.42,0.49),3,3,byrow=T)
P2=P%*%P
P3=P2%*%P
#print(xtable(P),type="latex",floating=F,include.rownames=F,
#include.colnames=F,hline.after=NULL)
X01=c(0,1,0)
dist.X21 = t(X01)%*%P2
dist.X31 = t(X01)%*%P3
X02=c(1/3,1/3,1/3)
dist.X22 = t(X02)%*%P2
dist.X32 = t(X02)%*%P3
```

$$
\mathbf{P}=\begin{pmatrix}
  0.36 & 0.48 & 0.16 \\ 
  0.18 & 0.54 & 0.28 \\ 
  0.09 & 0.42 & 0.49 
  \end{pmatrix}.
$$
\end{block}

## Cadenas de Markov. Matriz de transición de $h$ pasos
* La \red{matriz de probabilidades de transición de $h$ pasos} para la transición de $h$ pasos sería la siguiente:
$$
\mathbf{P}^{(h)}=\begin{pmatrix}
p_{11}^{(h)}&p_{12}^{(h)}&\cdots &p_{1n}^{(h)}\\
p_{21}^{(h)}&p_{22}^{(h)}&\cdots &p_{2n}^{(h)}\\
\vdots &\vdots &\vdots &\vdots\\
p_{n1}^{(h)}&p_{n2}^{(h)}&\cdots &p_{nn}^{(h)}
\end{pmatrix},
$$
donde recordemos que $p_{ij}^{(h)}=P(X(h)=j|X(0)=i)$.

## Cadenas de Markov. Matriz de transición de $h$ pasos

* La \red{matriz de probabilidades de transición de $h$ pasos} es también una matriz \red{estocástica}:
$$
p_{i1}^{(h)}+p_{i2}^{(h)}+\cdots + p_{in}^{(h)}=1.
$$

* Por tanto el vector $(p_{i1}^{(h)},\ldots, p_{in}^{(h)})$ representaría la \red{función de masa de probabilidad} de la variable aleatoria discreta $X(h)|X(0)=i$.

## Cadenas de Markov. Cálculo de $\mathbf{P}^{(h)}$
* Empecemos con el cálculo de $\mathbf{P}^{(2)}$, es decir $p_{ij}^{(2)}$, las probabilidades de transición de $2$ pasos:
\begin{align*}
p_{ij}^{(2)}= & P(X(2)=j|X(0)=i)\\ = & \sum_{k=1}^n P(X(1)=k|X(0)=i)\cdot P(X(2)=j|X(1)=k) \\ = & \sum_{k=1}^n p_{ik}\cdot p_{kj} =(p_{i1},\ldots,p_{in})\cdot \begin{pmatrix}p_{1j}\\\vdots\\ p_{nj}\end{pmatrix}.
\end{align*}

## Cadenas de Markov. Cálculo de $\mathbf{P}^{(h)}$

* La probabilidad $p_{ij}^{(2)}$ se calcula como la \red{suma de probabilidades} de ir del \blue{estado} $i$ a $j$ pasando por el \blue{estado} $k$, para $k$ desde $1$ hasta $n$:
$$
i\longrightarrow k\longrightarrow j.
$$

* En resumen $\mathbf{P}^{(2)}=\mathbf{P}\cdot\mathbf{P}=\mathbf{P}^2.$

## Cadenas de Markov. Cálculo de $\mathbf{P}^{(h)}$

* En general, las \red{probabilidades de transición de $h$ pasos} se pueden calcular en función de las \red{probabilidades de transición de $h-1$ pasos} razonando de forma similar:
\begin{align*}
p_{ij}^{(h)}= & P(X(h)=j|X(0)=i)\\ = & \sum_{k=1}^n P(X(h-1)=k|X(0)=i)\cdot P(X(h)=j|X(h-1)=k) \\ = & \sum_{k=1}^n p_{ik}^{(h-1)}\cdot p_{kj} =(p_{i1}^{(h-1)},\ldots,p_{in}^{(h-1)})\cdot \begin{pmatrix}p_{1j}\\\vdots\\ p_{nj}\end{pmatrix}.
\end{align*}

* En conclusión, 
$$\mathbf{P}^{(h)}=\mathbf{P}^{(h-1)}\cdot\mathbf{P}=\mathbf{P}^{h-1}\cdot\mathbf{P}=\mathbf{P}^h.$$

## Cadenas de Markov. Ejemplo
\begin{block}{Ejemplo}
Calculemos las \red{probabilidades de transición de $2$ y $3$ pasos}:
\begin{align*}
\mathbf{P}^{(2)} = & \mathbf{P}\cdot \mathbf{P}=\begin{pmatrix}
  0.36 & 0.48 & 0.16 \\ 
  0.18 & 0.54 & 0.28 \\ 
  0.09 & 0.42 & 0.49 
  \end{pmatrix}\cdot \begin{pmatrix}
  0.36 & 0.48 & 0.16 \\ 
  0.18 & 0.54 & 0.28 \\ 
  0.09 & 0.42 & 0.49 
  \end{pmatrix} \\ = &  \begin{pmatrix}
  0.2304 & 0.4992 & 0.2704 \\ 
  0.1872 & 0.4956 & 0.3172 \\ 
  0.1521 & 0.4758 & 0.3721
  \end{pmatrix},\\
\mathbf{P}^{(3)} = & \mathbf{P}^{(2)}\cdot \mathbf{P}=\begin{pmatrix}
  0.2304 & 0.4992 & 0.2704 \\ 
  0.1872 & 0.4956 & 0.3172 \\ 
  0.1521 & 0.4758 & 0.3721
  \end{pmatrix}\cdot \begin{pmatrix}
  0.36 & 0.48 & 0.16 \\ 
  0.18 & 0.54 & 0.28 \\ 
  0.09 & 0.42 & 0.49 
  \end{pmatrix} \\ = &  \begin{pmatrix}
  0.197136 & 0.493728 &  0.309136 \\ 
  0.185148 & 0.490704 & 0.324148 \\ 
  0.173889 & 0.486222 & 0.339889
  \end{pmatrix}
\end{align*}
\end{block}

## Cadenas de Markov. Ejemplo en R

```{r}
P2=P%*%P
P2
P3=P2%*%P
P3
```

## Cadenas de Markov. Ejemplo en python

```{python}
P2 = np.dot(P,P)
P2
P3 = np.dot(P2,P)
P3
```

## Distribución de $X(h)$
* Dar la \red{distribución de la variable aleatoria} $X(h)$, es decir, la distribución de la transición al cabo de $h$ \blue{pasos} sería equivalente a dar la \red{función de probabilidad} que sería una tabla del tipo:
\begin{center}
\begin{tabular}{c|c|c|c|c|}
$X(h)$&$1$&$2$&$\cdots$&$n$\\\hline
&$P_h(1)$&$P_h(2)$&$\cdots$&$P_h(n)$\\\hline
\end{tabular}
\end{center}

*
\begin{align*}
P_h(j)= & P(X(h)=j)=\sum_{k=1}^n P(X(0)=k)\cdot P(X(j)=j|X(0)=k)\\ = & \sum_{k=1}^n P_0(k)\cdot p_{kj}^{(h)}=(P_0(1),\ldots,P_0(n))\cdot\begin{pmatrix}p_{1j}^{(h)}\\\vdots\\ p_{nj}^{(h)}\end{pmatrix}
\end{align*}

## Distribución de $X(h)$

* La expresión nos da la \red{función de probabilidad} de $X(h)$ en función de la \red{función de probabilidad} de $X(0)$ y de las \red{probabilidades de transición} $\mathbf{P}^{(h)}$.

* Matricialmente $\mathbf{P}_h=\mathbf{P}_0\cdot \mathbf{P}^h.$


## Distribución de $X(h)$
\begin{block}{Ejemplo anterior}
Vamos a calcular la distribución de los usuarios conectados al cabo de $2$ y $3$ minutos en dos casos:
\begin{itemize}
\item Suponiendo que inicialmente hay un usuario conectado.
\item Suponiendo que el número de usuarios conectados inicialmente es equiprobable.
\end{itemize}


\end{block}

## Distribución de $X(h)$
\begin{block}{Ejemplo anterior}
En el primer caso, la \red{función de probabilidad} de $X(0)$ será $X(0)=(0,1,0)$.

La \red{función de probabilidad} de los usuarios conectados al cabo de $2$ y $3$ minutos será:
\begin{align*}
\mathbf{P}_2 = &(0,1,0)\cdot\mathbf{P}^{(2)}=(0,1,0)\cdot\begin{pmatrix}
  0.2304 & 0.4992 & 0.2704 \\ 
  0.1872 & 0.4956 & 0.3172 \\ 
  0.1521 & 0.4758 & 0.3721
  \end{pmatrix}\\ = & \begin{pmatrix}`r dist.X21[1]`\\ `r dist.X21[2]`\\ `r dist.X21[3]`\end{pmatrix},
\end{align*}
\end{block}

## Distribución de $X(h)$

\begin{block}{Ejemplo anterior}
\begin{align*}
  \mathbf{P}_3 = &(0,1,0)\cdot\mathbf{P}^{(3)}=(0,1,0)\cdot\begin{pmatrix}
  0.197136 & 0.493728 &  0.309136 \\ 
  0.185148 & 0.490704 & 0.324148 \\ 
  0.173889 & 0.486222 & 0.339889
  \end{pmatrix}\\ = & \begin{pmatrix}`r dist.X31[1]`\\ `r dist.X31[2]`\\ `r dist.X31[3]`\end{pmatrix}.
\end{align*}
\end{block}



## Distribución de $X(h)$
\begin{block}{Ejemplo anterior}
En el segundo caso, la \red{función de probabilidad} de $X(0)$ será $X(0)=\left(\frac{1}{3},\frac{1}{3},\frac{1}{3}\right)$.

La \red{función de probabilidad} de los usuarios conectados al cabo de $2$ y $3$ minutos será:
\begin{align*}
\mathbf{P}_2 = &\left(\frac{1}{3},\frac{1}{3},\frac{1}{3}\right)\cdot\mathbf{P}^{(2)}=\left(\frac{1}{3},\frac{1}{3},\frac{1}{3}\right)\cdot\begin{pmatrix}
  0.2304 & 0.4992 & 0.2704 \\ 
  0.1872 & 0.4956 & 0.3172 \\ 
  0.1521 & 0.4758 & 0.3721
  \end{pmatrix}\\ = & \begin{pmatrix}`r dist.X22[1]`\\ `r dist.X22[2]`\\ `r dist.X22[3]`\end{pmatrix},
\end{align*}
\end{block}


## Distribución de $X(h)$
\begin{block}{Ejemplo anterior}
\begin{align*}
  \mathbf{P}_3 = &\left(\frac{1}{3},\frac{1}{3},\frac{1}{3}\right)\cdot\mathbf{P}^{(3)}\\ = & \left(\frac{1}{3},\frac{1}{3},\frac{1}{3}\right)\cdot\begin{pmatrix}
  0.197136 & 0.493728 &  0.309136 \\ 
  0.185148 & 0.490704 & 0.324148 \\ 
  0.173889 & 0.486222 & 0.339889
  \end{pmatrix}\\ = & \begin{pmatrix}`r dist.X32[1]`\\ `r dist.X32[2]`\\ `r dist.X32[3]`\end{pmatrix}.
\end{align*}
\end{block}

## Cadenas de Markov. Ejemplo en R

```{r}
X01=c(0,1,0)
(dist.X21 = t(X01)%*%P2)
(dist.X31 = t(X01)%*%P3)
```


## Cadenas de Markov. Ejemplo en python

```{python}
X01= np.matrix([0,1,0])
dist_X21 = X01*P2
dist_X21
dist_X31 = X01*P3
dist_X31
```

## Cadenas de Markov. Ejemplo en R

```{r}
X02=c(1/3,1/3,1/3)
(dist.X22 = t(X02)%*%P2)
(dist.X32 = t(X02)%*%P3)
```

## Cadenas de Markov. Ejemplo en python

```{python}
X02= np.matrix([1/3,1/3,1/3])
dist_X22 = X02*P2
dist_X22
dist_X32 = X02*P3
dist_X32
```



## Distribución en el equilibrio
* La \red{distribución de la cadena de Markov en el equilibrio} nos dice cuál es la distribución de $X(h)$, $\mathbf{P}_h$, al cabo de un número muy grande de \blue{transiciones} $h$.

* Es decir, definimos la \red{distribución de la cadena de Markov en el equilibrio} $\pi(x)$ de la forma siguiente:
$$
\pi(x)=\lim_{h\to\infty}\mathbf{P}_h(x),\ x=1,\ldots, n.
$$

* La \red{distribución de la cadena de Markov en el equilibrio} puede interpretarse como el porcentaje de "tiempo" que una persona pasa en cada estado $x$ suponiendo que realiza un camino aleatorio por la cadena según la \red{matriz de transición de probabilidades.}

## Distribución en el equilibrio
* Vamos a dar una forma de calcular $\pi(x)$ la \red{distribución de la cadena de Markov en el equilibrio.}

* Podemos escribir que \red{la función de probabilidad} de $X(h+1)$, $\mathbf{P}_{h+1}$ puede escribirse como:
$$
\mathbf{P}_h=\mathbf{P}_h\cdot \mathbf{P}.
$$

* Si hacemos $h\to \infty$ en la expresión anterior, obtenemos:
$$
\pi=\pi\cdot \mathbf{P}.
$$

* Entonces se dice que la \red{distribución de la cadena de Markov en el equilibrio} $\pi(x)$ es un \red{vector propio} de \red{valor propio} $1$ \blue{por la izquierda} de la \red{matriz de transición de probabilidades} $\mathbf{P}$.

## Distribución en el equilibrio
* En resumen, para calcular la \red{distribución de la cadena de Markov en el equilibrio} $\pi(x)$, hay que resolver:
\begin{align*}
\pi \mathbf{P} = & \pi,\\
\sum_{k=1}^n \pi = & 1.
\end{align*}

* Hallar un \red{vector propio} \blue{por la izquierda} de la matriz $\mathbf{P}$ es equivalente a hallar un \red{vector propio} \blue{por la derecha} de la matriz $\mathbf{P}^\top$ ya que si:
$$
\pi \mathbf{P} =  \pi,\ \Rightarrow \mathbf{P}^\top\cdot\pi^\top = \pi^\top.
$$

## Distribución en el equilibrio
```{r,echo=FALSE}
equilibrio=eigen(t(P))$vectors[,1]/sum(eigen(t(P))$vectors[,1])
```

\begin{block}{Ejemplo}
El vector propio de valor propio $1$ de la matriz $\mathbf{P}^\top$ cuyas componentes suman $1$ y son positivas vale:
$$
(`r equilibrio[1]`, `r equilibrio[2]`, `r equilibrio[3]`).
$$
El vector anterior sería la \red{distribución de probabilidad en el equilibrio} de la cadena de Markov considerada.
\end{block}


## Distribución en el equilibrio en R

\begin{block}{Ejemplo}
Para hallar la \red{distribución de probabilidad en el equilibrio} en R haríamos lo siguiente:
\end{block}

```{r}
(vectores.propios = eigen(t(P))$vectors)
```


## Distribución en el equilibrio en R

```{r}
(vector.propio.vap.1=vectores.propios[,1])
(equilibrio = 
    vector.propio.vap.1/sum(vector.propio.vap.1))
```

## Distribución en el equilibrio en python

```{python}
from numpy import linalg as LA
vap, vep = LA.eig(P.T)
vep
equilibrio = vep[:,0]/vep[:,0].sum()
equilibrio
```

