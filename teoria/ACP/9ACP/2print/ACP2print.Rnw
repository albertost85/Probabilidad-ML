\documentclass[12pt,t]{beamer}
% \documentclass[t]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[catalan]{babel}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{amsfonts,amssymb,amsmath,amsthm, wasysym, multirow}
\usepackage{listings}
\lstset{language=R}
\usepackage[T1]{fontenc}        
\usepackage{pgf}
%\usepackage{epsdice}
\usepackage{pgfpages}


\usepackage{tikz}
\usetikzlibrary{arrows,shapes,plotmarks,backgrounds,trees}
\newcounter{nivell}
\setcounter{nivell}{0}
\newcommand{\nounivell}{%
  \addtocounter{nivell}{1}}
\newcommand{\nvl}{\value{nivell}}
\tikzstyle{hyb}=[rectangle,fill=green!50,draw]%,minimum size=0.5mm]
\tikzstyle{tree}=[circle,fill=green!50,draw]%,minimum size=0.5mm]

\tikzstyle{hybgr}=[rectangle,fill=green!50,draw,minimum size=5mm]
\tikzstyle{treegr}=[circle,fill=green!50,draw,minimum size=5.5mm]

\tikzstyle{hybnou}=[rectangle,fill=red!50,draw]%,minimum size=0.5mm]
\tikzstyle{trenou}=[circle,fill=red!50,draw]%,minimum size=0.5mm]

\tikzstyle{treered}=[circle,fill=red!50,draw]%,minimum size=6mm]


\newcommand{\etq}[1]{%
\draw (#1) node {\scriptsize $#1$};
}
\newcommand{\etqb}[2]{%
\draw (#1) node {\scriptsize $#1_{#2}$};
}
%\usetikzlibrary{arrows,shapes,plotmarks,backgrounds,trees,positioning}
%\usetikzlibrary{decorations.pathmorphing,calc,snakes}
%\usepackage{marvosym}
%
\usepackage{pgfpages}
\pgfpagesuselayout{4 on 1}[a4paper,border shrink=5mm,landscape]
\setbeamertemplate{footline}[frame number]
\usecolortheme{sidebartab}
\useinnertheme[shadow]{rounded}
% \useoutertheme[footline=empty,subsection=true,compress]{infolines}
% \useoutertheme[footline=empty,subsection=true,compress]{miniframes}
% \usefonttheme{serif}

\setbeamertemplate{caption}[numbered]
\setbeamertemplate{navigation symbols}{}


\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\renewcommand{\emph}[1]{{\color{red}#1}}

\setbeamertemplate{frametitle}
{\begin{centering}
\medskip
\color{blue}
\textbf{\insertframetitle}
\medskip
\end{centering}
}
\usecolortheme{rose}
\usecolortheme{dolphin}
\mode<presentation>


\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\KK}{\mathbb{K}}
\newcommand{\MM}{\mathcal{M}}
%\newcommand{\dbinom}{\displaystyle\binom}

\newcommand{\limn}{{\displaystyle \lim_{n\to\infty}}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\def\tendeix{{\displaystyle\mathop{\longrightarrow}_{\scriptscriptstyle
n\to\infty}}}

\newcommand{\matriu}[1]{\left(\begin{matrix} #1 \end{matrix}\right)}

% \newcommand{\qed}{\hbox{}\nobreak\hfill\vrule width 1.4mm height 1.4mm depth 0mm
%     \par \goodbreak \smallskip}
%
% %
\theoremstyle{plain}
\newtheorem{teorema}{Teorema}
\newtheorem{prop}{Proposició}
\newtheorem{cor}{Coro\l.lari}
\theoremstyle{definition}
\newtheorem{exemple}{Exemple}
\newtheorem{defin}{Definició}
\newtheorem{obs}{Observació}

\newcounter{seccions}
\newcommand{\seccio}[1]{\addtocounter{seccions}{1}
\medskip\par\noindent\emph{\theseccions.
#1}\smallskip\par }

\newcommand{\EM}{\Omega}
\newcommand{\PP}{\mathcal{P}}

\title[\red{Matemàtiques II}]{}
\author[]{}
\date{}



\begin{document}
\beamertemplatedotitem

\lstset{backgroundcolor=\color{green!50}}
\lstset{breaklines=true}
\lstset{basicstyle=\ttfamily}
\lstset{extendedchars=true}
\lstset{showstringspaces=false}

\begin{frame}
\vfill
\begin{center}
\gray{\LARGE Anàlisi de Components Principals}
\end{center}
\vfill
\end{frame}
\section{Anàlisi de Components Principals}
\section{Introducció}
\begin{frame}
\frametitle{Introducció}

El problema central de l'anàlisi de dades és la \red{reducció de la dimensionalitat.}
\medskip

És a dir, si és possible descriure amb precisió els valors de les $p$ variables per un petit subconjunt $r <p$ 
d'aquestes variables amb una pèrdua mínima d'informació.
\bigskip

Aquest és l'objectiu de l'\red{anàlisi de components principals}: donades $n$ observacions de $p$ variables, 
(\blue{taula de dades}) s'analitza si és possible representar aquesta informació amb menys variables.


\end{frame}

\begin{frame}
\frametitle{Introducció}
Dit de forma més explícita, volem transformar les variables de la nostra taula de dades en unes noves variables 
anomenades \red{components principals} que siguin incorrelacionades entre sí i que siguin combinació lineal de les 
\blue{variables originals.}
\medskip

Si el nombre de variables noves és més petit que les \blue{variables originals}, hi haurà una pèrdua d'informació. 
\medskip

Volem que aquesta pèrdua sigui mínima en el sentit de que les \red{components principals} heretin la màxima 
\red{variabilitat} de les \blue{variables originals.}
\end{frame}

\iffalse
\begin{frame}
\frametitle{Taula de dades}
Suposem que la nostra \blue{taula de dades} és la següent:
\begin{center}
\begin{tabular}{c|cccc|c|}
Ind. & $x_1$ & $x_2$ & $\ldots$ & $x_p$ & $v$\\
  \hline
$1$  & & & & &  \\
 $2$  & & & & &  \\
 $3$ & & & & &  \\
 $\vdots$   & & & & &  \\
  $n$  & & & & &  \\ \hline
   $s$  & & & & &  \multicolumn{1}{|c|}{}\\
  \hline
\end{tabular}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Taula de dades}
\begin{itemize}
\item Les variables $x_1,\ldots, x_p$ són les nostres \blue{variables originals.}
\item Els individus $1,2,\ldots, n$ són els individus de la nostra \blue{taula de dades.}
\item La variables $v$ es diu variable \red{de perfil o explicativa} i és una variable que intentem explicar a partir de 
la nostra \blue{taula de dades.}
\item L'individu  $s$ és un individu \red{suplementari} o \red{il·lustratiu} que algunes vegades intenta representar a 
tots els individus de la \blue{taula de dades}.
\item Hi pot haver més d'una variable \red{explicativa} o més d'un individu \red{il·lustratiu}.


\end{itemize}
\end{frame}
\fi


\section{Components principals}
\begin{frame}
\frametitle{Components principals}

Anomenarem $X_1,\ldots,X_p$ a les nostres \blue{variables originals} i \red{$CP_1,\ldots,CP_r$} a les variables 
\red{components principals} on $r\leq p$.
\medskip

Volem calcular una matriu ${\Lambda}$ tal que:
\[
\mathbf{CP}={\Lambda}\mathbf{X},
\]
on $\mathbf{CP}=(CP_1,\ldots,CP_r)^\top$, $\mathbf{X}=(X_1,\ldots,X_p)^\top$ i 
\[
{\Lambda}=\begin{pmatrix}
\alpha_{11}&\cdots & \alpha_{1p}\\
\alpha_{21}&\cdots & \alpha_{2p}\\
\cdots & \cdots & \cdots \\
\alpha_{r1}&\cdots & \alpha_{rp}
\end{pmatrix}.
\]
\end{frame}

\begin{frame}
\frametitle{Components principals}
Escrit en components:
\begin{eqnarray*}
CP_1 &=& \alpha_{11} X_1+\cdots + \alpha_{1p} X_p, \\
CP_2 &=& \alpha_{21} X_1+\cdots + \alpha_{2p} X_p, \\
\vdots && \vdots \\
CP_r &=& \alpha_{11} X_1+\cdots + \alpha_{rp} X_p.
\end{eqnarray*}
\end{frame}

\begin{frame}
\frametitle{Components principals}
En la pràctica, sigui $\mathbf{X}$ la nostra matriu $n\times p$ que representa la taula de dades original on tenim $n$ 
individus i $p$ variables que suposarem centrada. O sigui, les mitjanes de les columnes de $\mathbf{X}$ són nul·les.
\bigskip

Volem obtenir una nova matriu $\mathbf{Y}$ $n\times r$ corresponent a les \red{components principals} tal que: 
$\mathbf{Y}=\mathbf{X}\cdot \Lambda^\top.$
\medskip

Escrit en components:
\[
y_{ki}=x_{k1} \alpha_{i1}+\cdots +x_{kp}\alpha_{ip},\mbox{ per } k=1,\ldots,n,i=1,\ldots, r.
\]
Ens adonem que la matriu $\mathbf{Y}$ també serà centrada.
\end{frame}

\section{Interpretació geomètrica}

\begin{frame}[fragile]
\frametitle{Interpretació geomètrica}
Suposem que $p=2$ i que el nostre \red{``núvol'' de punts} de la nostra taula de dades és el que mostra la figura:
\vspace*{-1cm}

<<fig=TRUE,fig.align="center",width=5,height=5,echo=FALSE>>=
n=20
m=10
#xx <- c(-10,10)
#yy <- c(-6,6)
#par(mar=c(0,0,0,0))
#plot(xx,yy,ann=F,type='n')
#dades=matrix(sample(1:m,2*n,replace=TRUE),n,2)
dades=read.table("dades.txt")
dades=as.matrix(dades)
Hn=matrix(-1/n,n,n)
diag(Hn)=rep(1-1/n,n)
dades.cent=Hn%*%dades
plot(dades.cent,asp=1,xlab="X",ylab="Y",xlim=c(-10,10),ylim=c(-5.5,5.5))
@
\end{frame}

\begin{frame}
\frametitle{Interpretació geomètrica}
{\small A continuació mostram les dues \red{components principals.} O sigui, les direccions on les projeccions de les 
dades tenen màxima variabilitat:}
\vspace*{-1cm}

<<fig=TRUE,fig.align="center",width=5,height=5,echo=FALSE>>=
n=20
m=10
#xx <- c(-10,10)
#yy <- c(-6,6)
#par(mar=c(0,0,0,0))
#plot(xx,yy,ann=F,type='n')
#dades=matrix(sample(1:m,2*n,replace=TRUE),n,2)
dades=read.table("dades.txt")
dades=as.matrix(dades)
Hn=matrix(-1/n,n,n)
diag(Hn)=rep(1-1/n,n)
dades.cent=Hn%*%dades
plot(dades.cent,asp=1,xlab="X",ylab="Y",xlim=c(-10,10),ylim=c(-5.5,5.5))
estudi.acp=prcomp(dades,scale.=FALSE)
direccions=estudi.acp$rotation
m.cov=t(dades.cent)%*%dades.cent
vaps=eigen(m.cov)$values
veps=eigen(m.cov)$vectors
mag=10
lines(c(0,mag*direccions[1,1]),c(0,mag*direccions[2,1]),col="red",lwd=3)
lines(c(0,mag*direccions[1,2]),c(0,mag*direccions[2,2]),col="red",lwd=3)
lines(c(0,-mag*direccions[1,1]),c(0,-mag*direccions[2,1]),col="red",lwd=3)
lines(c(0,-mag*direccions[1,2]),c(0,-mag*direccions[2,2]),col="red",lwd=3)
xaux=-6
text(xaux,direccions[2,1]*xaux/direccions[1,1]-1,"CP1",cex=1)
xaux=-1.5
text(xaux,direccions[2,2]*xaux/direccions[1,2]-2,"CP2",cex=1)
@
\end{frame}

\begin{frame}
\frametitle{Interpretació geomètrica}
{\small Si projectam en la direcció de la \red{primera component}, obtendrem les projeccions següents (punts blaus):}
\vspace*{-1cm}

<<fig=TRUE,fig.align="center",width=5,height=5,echo=FALSE>>=
n=20
m=10
#xx <- c(-10,10)
#yy <- c(-6,6)
#par(mar=c(0,0,0,0))
#plot(xx,yy,ann=F,type='n')
#dades=matrix(sample(1:m,2*n,replace=TRUE),n,2)
dades=read.table("dades.txt")
dades=as.matrix(dades)
Hn=matrix(-1/n,n,n)
diag(Hn)=rep(1-1/n,n)
dades.cent=Hn%*%dades
plot(dades.cent,asp=1,xlab="X",ylab="Y",xlim=c(-10,10),ylim=c(-5.5,5.5))
estudi.acp=prcomp(dades,scale.=FALSE)
direccions=estudi.acp$rotation
m.cov=t(dades.cent)%*%dades.cent
vaps=eigen(m.cov)$values
veps=eigen(m.cov)$vectors
mag=10
lines(c(0,mag*direccions[1,1]),c(0,mag*direccions[2,1]),col="red",lwd=3)
lines(c(0,mag*direccions[1,2]),c(0,mag*direccions[2,2]),col="red",lwd=3)
lines(c(0,-mag*direccions[1,1]),c(0,-mag*direccions[2,1]),col="red",lwd=3)
lines(c(0,-mag*direccions[1,2]),c(0,-mag*direccions[2,2]),col="red",lwd=3)
xaux=-6
text(xaux,direccions[2,1]*xaux/direccions[1,1]-1,"CP1",cex=1)
xaux=-1.5
text(xaux,direccions[2,2]*xaux/direccions[1,2]-2,"CP2",cex=1)
#arrows(0,0,mag*direccions[1,1],mag*direccions[2,1],col="red",lwd=3)
#arrows(0,0,mag*direccions[1,2],mag*direccions[2,2],col="red",lwd=3)
#arrows(0,0,-mag*direccions[1,1],-mag*direccions[2,1],col="red",lwd=3)
#arrows(0,0,-mag*direccions[1,2],-mag*direccions[2,2],col="red",lwd=3)
proj.ort=function(x0,y0,v1,v2,b){m=v2/v1;x1=(m*y0+x0-m*b)/(m^2+1);
                        y1=(m^2*y0+m*x0+b)/(m^2+1);return(c(x1,y1))}
#projecció sobre CP1
for (i in 1:dim(dades)[1]){
  aux=proj.ort(dades.cent[i,1],dades.cent[i,2],veps[1,1],veps[2,1],0)
  points(aux[1],aux[2],pch=20,col="blue")
  lines(c(dades.cent[i,1],aux[1]),c(dades.cent[i,2],aux[2]),lty=2)
}
@
\end{frame}

\begin{frame}
\frametitle{Interpretació geomètrica}
Això significa que la variància dels punts blaus és màxima en el sentit de que si haguéssim escollit una altra direcció 
o una altra recta i haguéssim projectat sobre aquesta segona recta, la variància de les projeccions hagués estar menor.
\medskip

Els punts blaus representen les coordenades que tenen els punts de la nostra taula de dades si haguéssim agafat com eix 
d'abscisses, l'eix de la \red{primera component} $CP_1$.
\end{frame}

\begin{frame}
\frametitle{Interpretació geomètrica}
{\small Si projectam en la direcció de la \red{segona component}, obtendrem les projeccions següents (punts verds):}
\vspace*{-1cm}

<<fig=TRUE,fig.align="center",width=5,height=5,echo=FALSE>>=
n=20
m=10
#xx <- c(-10,10)
#yy <- c(-6,6)
#par(mar=c(0,0,0,0))
#plot(xx,yy,ann=F,type='n')
#dades=matrix(sample(1:m,2*n,replace=TRUE),n,2)
dades=read.table("dades.txt")
dades=as.matrix(dades)
Hn=matrix(-1/n,n,n)
diag(Hn)=rep(1-1/n,n)
dades.cent=Hn%*%dades
plot(dades.cent,asp=1,xlab="X",ylab="Y",xlim=c(-10,10),ylim=c(-5.5,5.5))
estudi.acp=prcomp(dades,scale.=FALSE)
direccions=estudi.acp$rotation
m.cov=t(dades.cent)%*%dades.cent
vaps=eigen(m.cov)$values
veps=eigen(m.cov)$vectors
mag=10
lines(c(0,mag*direccions[1,1]),c(0,mag*direccions[2,1]),col="red",lwd=3)
lines(c(0,mag*direccions[1,2]),c(0,mag*direccions[2,2]),col="red",lwd=3)
lines(c(0,-mag*direccions[1,1]),c(0,-mag*direccions[2,1]),col="red",lwd=3)
lines(c(0,-mag*direccions[1,2]),c(0,-mag*direccions[2,2]),col="red",lwd=3)
xaux=-6
text(xaux,direccions[2,1]*xaux/direccions[1,1]-1,"CP1",cex=1)
xaux=-1.5
text(xaux,direccions[2,2]*xaux/direccions[1,2]-2,"CP2",cex=1)
#arrows(0,0,mag*direccions[1,1],mag*direccions[2,1],col="red",lwd=3)
#arrows(0,0,mag*direccions[1,2],mag*direccions[2,2],col="red",lwd=3)
#arrows(0,0,-mag*direccions[1,1],-mag*direccions[2,1],col="red",lwd=3)
#arrows(0,0,-mag*direccions[1,2],-mag*direccions[2,2],col="red",lwd=3)
proj.ort=function(x0,y0,v1,v2,b){m=v2/v1;x1=(m*y0+x0-m*b)/(m^2+1);
                        y1=(m^2*y0+m*x0+b)/(m^2+1);return(c(x1,y1))}
#projecció sobre CP2
for (i in 1:dim(dades)[1]){
  aux=proj.ort(dades.cent[i,1],dades.cent[i,2],veps[1,2],veps[2,2],0)
  points(aux[1],aux[2],pch=20,col="green")
  lines(c(dades.cent[i,1],aux[1]),c(dades.cent[i,2],aux[2]),lty=2)
}
@
\end{frame}


\begin{frame}
\frametitle{Components principals}
Condicions han de verificar les components principals:
\begin{block}{Han d'esser incorrelades.}
O sigui, $r_{CP_i,CP_j}=0$ o si $\mathbf{y}_i$ i $\mathbf{y_j}$ són les columnes $i$ i $j$ de la matriu $\mathbf{Y}$, 
$r_{\mathbf{y}_i,\mathbf{y}_j}=0$. Dit en altres paraules, la matriu de covariàncies o de correlacions de la taula de 
dades $\mathbf{Y}$ serà diagonal.
\end{block}
\end{frame}

\begin{frame}
\frametitle{Components principals}
Condicions han de verificar les components principals:
\begin{block}{Les variàncies de les \red{components principals} han de decréixer.}
O sigui, \[
\mbox{var}(CP_1)\geq \mbox{var}(CP_2)\cdots \geq \mbox{var}(CP_p).
\]
D'aquesta forma, la \red{component principal} $CP_1$ serà la que tengui més variabilitat de totes i per tant, la més 
important, $CP_2$, la segona més important i així successivament.
\end{block}
\end{frame}


\iffalse
\begin{frame}
\frametitle{Components principals}
Condicions han de verificar les components principals:
\begin{block}{Conservació de la variabilitat}
Si $r=p$ o si tenim tantes \red{components principals} com \blue{variables originals}, la \red{variabilitat} de les 
variables s'ha de conservar:
\[
\mbox{var}(X_1)+\cdots +\mbox{var}(X_p)=\mbox{var}(CP_1)+\cdots + \mbox{var}(CP_p),
\]
o escrit en funció de les matrius $\mathbf{X}$ i $\mathbf{Y}$,
\[
s_{x_1}^2+\cdots +s_{x_p}^2 = s_{y_1}^2+\cdots + s_{y_p}^2,
\]
on $s_{x_i}^2$ és la variància de la columna $i$-èssima de la matriu~$\mathbf{X}$ i $s_{y_i^2}$ és la variància de la 
columna $i$-èssima de la matriu~$\mathbf{Y}.$
\end{block}
\end{frame}
\fi 

\iffalse
\section{Tipus d'Anàlisi de Components Principals}

\begin{frame}
\frametitle{Tipus d'Anàlisi de Components Principals}
Depenent de com sigui la matriu de dades de les \blue{originals} tenim dos tipus d'anàlisi de components principals:
\begin{itemize}
\item Si la matriu de dades $\mathbf{X}$ és centrada, l'\red{anàlisi de components principals} es diu anàlisi de 
components principals (ACP) sobre la \red{matriu de covariàncies} ja que farem servir aquesta matriu per realitzar 
l'anàlisi.
\item Si la matriu de dades $\mathbf{X}$ és tipificada, o sigui, les seves columnes, a més de tenim mitjana zero, tenen 
variància~$1$, l'\red{anàlisi de components principals} es diu anàlisi de components principals sobre la \red{matriu de 
correlacions} ja que  farem servir aquesta matriu per realitzar l'anàlisi.
\end{itemize}
\end{frame}
\fi

\section{ACP sobre la matriu de covariàncies}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
Considerem $\mathbf{X}$ $n\times p$ la nostre matriu de dades que suposarem centrada on tenim $n$ individus i $p$ 
variables. Si no ho fos, l'haurem de centrar.
\bigskip

Sigui $\mathbf{S}$ $p\times p$ la matriu de covariàncies de $\mathbf{X}$.
\medskip

Recordem que $\mathbf{S}$ es calcula com:
\[
\mathbf{S}=\frac{1}{n}\mathbf{X}^\top\cdot\mathbf{X}.
\]

\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
Siguin $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p$ els valors propis de la matriu~$\mathbf{S}$ en ordre 
creixent.
\medskip

Siguin $\mathbf{v}_1,\mathbf{v}_2,\ldots,\mathbf{v}_p$ els corresponents vectors propis que suposarem ortogonals i 
normalitzats. Això és, suposarem que són perpendiculars dos a dos i tenen norma euclídea unitat.
\medskip

Sigui $\mathbf{V}$ la matriu de vectors propis que té els vectors anteriors per columnes.
\medskip

Aleshores la matriu $\Lambda$ és la transposada de la matriu~$\mathbf{V}$: 
\[
\Lambda=\mathbf{V}^\top.
\]
\end{frame}

\begin{frame}[fragile]
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}
Considerem la següent matriu de dades:
<<echo=F>>=
library(xtable)
X=matrix(c(1,1,2,3,-1,0,3,0,3,3,0,1),4,3)
n=4
H4=matrix(-1/n,n,n)
diag(H4)=rep(1-1/n,n)
Xtilde=H4%*%X
@
\[
\mathbf{X}= 
\left(
<<echo=F,results=tex>>=
print(xtable(X,digits=0),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]

\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}
Com que la matriu no està centrada, primer la centram:
{\footnotesize
\begin{eqnarray*}
\tilde{\mathbf{X}}& = & \mathbf{H}_4\mathbf{X} \\ &=&
\left(
<<echo=F,results=tex>>=
print(xtable(H4,digits=2),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)\cdot
\left(
<<echo=F,results=tex>>=
print(xtable(X,digits=0),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right) \\ &=&
\left(
<<echo=F,results=tex>>=
print(xtable(Xtilde,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\end{eqnarray*}
}
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}
{
La matriu de covariàncies serà:
<<echo=F>>=
m.cov=(1/n)*t(Xtilde)%*%Xtilde
vaps=eigen(m.cov)$values
veps=eigen(m.cov)$vectors
@
\[
\mathbf{S}=\frac{1}{4}\tilde{\mathbf{X}}^\top\tilde{\mathbf{X}}=
\left(
<<echo=F,results=tex>>=
print(xtable(as.matrix(m.cov),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
Els valors propis de la matriu de covariàncies anterior són:
\[
<<echo=F,results=tex>>=
print(xtable(t(as.matrix(vaps)),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\]
}
\end{exemple}
\end{frame}


\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}
Els corresponents vectors propis (per columnes) associats als vectors propis anteriors són:
\[
<<echo=F,results=tex>>=
print(xtable(veps,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\]
Ens adonem que els vectors anteriors són ortogonals i estan normalitzats.


\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}
{\small
Així la matriu que ens canviarà de \blue{variables originals} a \red{components principals} serà la transposada dels 
vectors propis:
\[
\Lambda=
\left(
<<echo=F,results=tex>>=
print(xtable(t(veps),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
Per tant, la matriu de dades en les noves variables serà:
<<echo=F>>=
CP=Xtilde%*%veps
@
\[
\mathbf{Y}=\tilde{\mathbf{X}}\Lambda^\top = \tilde{\mathbf{X}}\mathbf{V}=
\left(
<<echo=F,results=tex>>=
print(xtable(CP,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
}
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}
<<echo=F>>=
cp.cov = cov(CP)*(n-1)/n
@
Si calculam la matriu de covariàncies de les \red{components principals} val:
\[
\mathbf{S}_{CP}=\left(
<<echo=F,results=tex>>=
print(xtable(cp.cov,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
Ens adonam que és diagonal, per tant, les covariàncies entre variables diferents són nul·les i en la diagonal hi ha les 
variàncies de les \red{components principals} que estan en ordre creixent. Us sonen aquests valors?
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}
Efectivament, són els valors propis de la matriu de covariàncies de les \blue{variables originals}~$\mathbf{S}$.
\bigskip

Comprovem que la variabilitat s'ha conservat. 
\medskip

La variabilitat de les \blue{variables originals} serà la suma dels valors de la diagonal de la matriu~$\mathbf{S}$:
\[
\Sexpr{round(diag(m.cov)[1],3)} + \Sexpr{round(diag(m.cov)[2],3)} + \Sexpr{round(diag(m.cov)[3],3)} = 
\Sexpr{round(sum(diag(m.cov)),3)}.
\]
\end{exemple}
\end{frame}


\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}
La variabilitat de les \red{components principals} serà la suma dels valors de la diagonal de la 
matriu~$\mathbf{S}_{CP}$:
\[
\Sexpr{round(diag(cp.cov)[1],3)} + \Sexpr{round(diag(cp.cov)[2],3)} + \Sexpr{round(diag(cp.cov)[3],3)} = 
\Sexpr{round(sum(diag(cp.cov)),3)}.
\]
Podem observar que les dues variabilitats coincideixen.
\medskip

La primera \red{component principal} (1a. columna de la matriu $\mathbf{CP}$) hereta el 
$\frac{\Sexpr{round(diag(cp.cov)[1],3)}}{\Sexpr{round(sum(diag(cp.cov)),3)}}\cdot 100\% = 
\Sexpr{round(diag(cp.cov)[1]*100/sum(diag(cp.cov)),3)}\%$ de la variabilitat total.

Les dues primeres \red{components principals} hereten el 
$\frac{\Sexpr{round(diag(cp.cov)[1],3)} + \Sexpr{round(diag(cp.cov)[2],3)}}{\Sexpr{round(sum(diag(cp.cov)),3)}}\cdot 
100\% = 
\Sexpr{round(sum(diag(cp.cov)[1:2])*100/sum(diag(cp.cov)),3)}\%$ de la variabilitat total.
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}
Facem un exemple més complet.
La taula següent ens dóna l'edat en dies ($x_1$), l'alçada al néixer en cm. ($x_2$), el seu pes en kg. en néixer ($x_3$) 
i l'augment en tant per cent del seu pes actual respecte el seu pes en néixer ($x_4$) de $9$ nens i nenes recent nats. 
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}
{\small 
\begin{center}\begin{tabular}{|c|c|c|c|c|}\hline
$x_1$ & $x_2$ & $x_3$ & $x_4$&Sexe\\\hline
78&48.2&2.75&29.5&Nina\\ 69&45.5&2.15&26.3&Nina\\
77&46.3&4.41&32.2&Nina\\ 88&49&5.52&36.5&Nin\\ 67&43&3.21&27.2&Nina\\
80&48&4.32&27.7&Nina\\ 74&48&2.31&28.3&Nina\\ 94&53&4.3&30.3&Nin\\
102&58&3.71&28.7&Nin
\\\hline\end{tabular}\end{center}
Hem afegit una variable més (sexe de l'infant). Ens demanem si aquestes $4$ variables són capaces d'explicar o de predir 
la variable anterior. 
}
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}
La matriu de dades centrada seria:
<<echo=F>>=
nens=matrix(c(78,48.2,2.75,29.5, 69,45.5,2.15,26.3,
77,46.3,4.41,32.2, 88,49,5.52,36.5,67,43,3.21,27.2,
80,48,4.32,27.7, 74,48,2.31,28.3, 94,53,4.3,30.3,
102,58,3.71,28),9,4,byrow=T)
n=9
H9=matrix(-1/n,n,n)
diag(H9)=rep(1-1/n,n)
nens.cent=H9%*%nens
cov.nens=(1/n)*t(nens.cent)%*%nens.cent
vaps.nens=eigen(cov.nens)$values
veps.nens=eigen(cov.nens)$vectors
@
\[
\tilde{\mathbf{X}}=\left(
<<echo=F,results=tex>>=
print(xtable(nens.cent,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]


\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}

La matriu de covariàncies serà:
{\small\[
\mathbf{S}=\frac{1}{9}\tilde{\mathbf{X}}^\top\tilde{\mathbf{X}}=
\left(
<<echo=F,results=tex>>=
print(xtable(as.matrix(cov.nens),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]}
Els valors propis de la matriu de covariàncies anterior són:
\[
<<echo=F,results=tex>>=
print(xtable(t(as.matrix(vaps.nens)),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\]

\end{exemple}
\end{frame}


\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}
Els corresponents vectors propis (per columnes) associats als vectors propis anteriors són:
\[
<<echo=F,results=tex>>=
print(xtable(veps.nens,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\]
Ens adonem que els vectors anteriors són ortogonals i estan normalitzats.


\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}
{
Així la matriu que ens canviarà de \blue{variables originals} a \red{components principals} serà la transposada dels 
vectors propis:
\[
\Lambda=
\left(
<<echo=F,results=tex>>=
print(xtable(t(veps.nens),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
}
\end{exemple}
\end{frame}

<<echo=F>>=
signe=function(x){ifelse(x>0,"+","-")}
signe0=function(x){ifelse(x>0,"","-")}
@

\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}
Les expressions de les \red{components principals} en funció de les \blue{variables originals} són:
\begin{eqnarray*}
CP_1 &=& \Sexpr{signe0(veps.nens[1,1])}\Sexpr{round(abs(veps.nens[1,1]),3)}X_1 
\Sexpr{signe(veps.nens[2,1])}\Sexpr{round(abs(veps.nens[2,1]),3)}X_2
\Sexpr{signe(veps.nens[3,1])}\Sexpr{round(abs(veps.nens[3,1]),3)}X_3 
\Sexpr{signe(veps.nens[4,1])}\Sexpr{round(abs(veps.nens[4,1]),3)}X_4
\\
CP_2 &=& \Sexpr{signe0(veps.nens[1,2])}\Sexpr{round(abs(veps.nens[1,2]),3)}X_1
\Sexpr{signe(veps.nens[2,2])}\Sexpr{round(abs(veps.nens[2,2]),3)}X_2
\Sexpr{signe(veps.nens[3,2])}\Sexpr{round(abs(veps.nens[3,2]),3)}X_3 
\Sexpr{signe(veps.nens[4,2])}\Sexpr{round(abs(veps.nens[4,2]),3)}X_4
\\
CP_3 &=& \Sexpr{signe0(veps.nens[1,3])}\Sexpr{round(abs(veps.nens[1,3]),3)}X_1
\Sexpr{signe(veps.nens[2,3])}\Sexpr{round(abs(veps.nens[2,3]),3)}X_2
\Sexpr{signe(veps.nens[3,3])}\Sexpr{round(abs(veps.nens[3,3]),3)}X_3 
\Sexpr{signe(veps.nens[4,3])}\Sexpr{round(abs(veps.nens[4,3]),3)}X_4 \\
CP_4 &=& \Sexpr{signe0(veps.nens[1,4])}\Sexpr{round(abs(veps.nens[1,4]),3)}X_1
\Sexpr{signe(veps.nens[2,4])}\Sexpr{round(abs(veps.nens[2,4]),3)}X_2
\Sexpr{signe(veps.nens[3,4])}\Sexpr{round(abs(veps.nens[3,4]),3)}X_3 
\Sexpr{signe(veps.nens[4,4])}\Sexpr{round(abs(veps.nens[4,4]),3)}X_4
\end{eqnarray*}
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}
{\small
Per tant, la matriu de dades en les noves variables serà:
<<echo=F>>=
CP.nens=nens.cent%*%veps.nens
@
\[
\mathbf{Y}=\tilde{\mathbf{X}}\Lambda^\top = \tilde{\mathbf{X}}\mathbf{V}=
\left(
<<echo=F,results=tex>>=
print(xtable(CP.nens,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
}
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}
<<echo=F>>=
cp.nens.cov = cov(CP.nens)*(n-1)/n
@
Si calculam la matriu de covariàncies de les \red{components principals} val:
\[
\mathbf{S}_{CP}=\left(
<<echo=F,results=tex>>=
print(xtable(cp.nens.cov,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
Igual que passava en l'exemple anterior, és diagonal, com esperàvem i en la diagonal hi surten els valors propis de la 
matriu de covariàncies de les \blue{dades originals.}
\end{exemple}
\end{frame}


\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}
Comprovem que la variabilitat s'ha conservat. 
\medskip

La variabilitat de les \blue{variables originals} serà la suma dels valors de la diagonal de la matriu~$\mathbf{S}$:
\[
\Sexpr{round(diag(cov.nens)[1],3)} + \Sexpr{round(diag(cov.nens)[2],3)} + 
\Sexpr{round(diag(cov.nens)[3],3)} + \Sexpr{round(diag(cov.nens)[4],3)} = \Sexpr{round(sum(diag(cov.nens)),3)}.
\]
La variabilitat de les \red{components principals} serà la suma dels valors de la diagonal de la 
matriu~$\mathbf{S}_{CP}$:
\[
\Sexpr{round(diag(cp.nens.cov)[1],3)} + \Sexpr{round(diag(cp.nens.cov)[2],3)} + \Sexpr{round(diag(cp.nens.cov)[3],3)} 
+\Sexpr{round(diag(cp.nens.cov)[4],3)}= \Sexpr{round(sum(diag(cp.nens.cov)),3)}.
\]
Podem observar que les dues variabilitats coincideixen.
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}

La primera \red{component principal} (1a. columna de la matriu $\mathbf{CP}$) hereta el 
$\frac{\Sexpr{round(diag(cp.nens.cov)[1],3)}}{\Sexpr{round(sum(diag(cp.nens.cov)),3)}}\cdot 100\% = 
\Sexpr{round(diag(cp.nens.cov)[1]*100/sum(diag(cp.nens.cov)),3)}\%$ de la variabilitat total.

Les dues primeres \red{components principals} hereten el 
$\frac{\Sexpr{round(diag(cp.nens.cov)[1],3)} + 
\Sexpr{round(diag(cp.nens.cov)[2],3)}}{\Sexpr{round(sum(diag(cp.nens.cov)),3)}}\cdot 100\% = 
\Sexpr{round(sum(diag(cp.nens.cov)[1:2])*100/sum(diag(cp.nens.cov)),3)}\%$ de la variabilitat total.
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}
Això ens fa pensar que si només tenim en compte les dues primeres components, podem fer un gràfic on hi estiguin 
representats tots els nens dibuixant de blau els nens i de vermell les nenes.
\medskip

Comprovem que les dues primeres components separen bé els nens i les nens. Concloem que la nostra taula de dades 
``explica'' la variable sexe.
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de covariàncies}}
\begin{exemple}
\vspace*{-1cm}

<<echo=F,fig=T,width=5,height=5>>=
sexe=c("nina","nina","nina","nin","nina","nina","nina","nin","nin")
plot(CP.nens[,1],CP.nens[,2],xlab="CP1",ylab="CP2",col=ifelse(sexe=="nin","blue","red"))
@
\end{exemple}
\end{frame}

\section{Propietats de l'ACP sobre la matriu de covariàncies}
\begin{frame}
\frametitle{Propietats de l'ACP sobre la \red{matriu de covariàncies}}
Recordem que la matriu $\mathbf{S}$ és la matriu de covariàncies de les \blue{variables originals} i $\mathbf{S}_{CP}$ 
és la matriu de covariàncies de les \red{components principals.}
\medskip

La diagonal de la matriu $\mathbf{S}$ està formada per les variàncies de les \blue{variables originals} $s_i^2$, 
$i=1,\ldots,p$.
\medskip

Definim la \red{variància total} de la nostra taula de dades com la suma de les variàncies o la traça de la 
matriu~$\mathbf{S}$:
\[
\mbox{Variància Total}=tr(\mathbf{S})=\sum_{i=1}^p s_i^2.
\]
\end{frame}


\begin{frame}
\frametitle{Propietats de l'ACP sobre la \red{matriu de covariàncies}}
\begin{itemize}
\item La variància de la $i$-èssima \red{component principal} és el \red{valor propi} $i$-èssim de la matriu de 
covariàncies~$\mathbf{S}$: $\mbox{Var} (CP)_i =\lambda_i.$
\item Es conserva la variància total. O sigui, la variància total de les \blue{variables originals} i de les 
\red{components principals} és la mateixa:
\[
\sum_{i=1}^p \mbox{var}(X_i) = \sum_{i=1}^p \mbox{var}(CP_i) =\sum_{i=1}^p\lambda_i.
\]

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Propietats de l'ACP sobre la \red{matriu de covariàncies}}
\begin{itemize}
\item Les \red{components principals} són incorrelades. O, dit, en altres paraules, la seva matriu de covariàncies és 
diagonal:
$$\mathbf{S}_{CP}=\left(\begin{array}{cccc}
\lambda_1& 0 &\ldots &  0\\
0& \lambda_{2}&\ldots & 0\\
\vdots & \vdots & & \vdots\\
0 & 0&\ldots &  \lambda_{p}
\end{array}
\right)$$

\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Propietats de l'ACP sobre la \red{matriu de covariàncies}}
{\small
\begin{itemize}
\item Donada una taula de dades, definim la \red{variància generalitzada} com el determinant de la matriu de 
covariàncies. Aleshores, la \red{variància generalitzada} de les \blue{variables originals} i de les \red{components 
principals} coincideix:
\[
\mbox{det}(\mathbf{S}) =\mbox{det}(\mathbf{S}_{CP}) =\lambda_1\cdots \lambda_p.
\]
\item La proporció de variància explicada per la \red{component $j$-èssima} és: 
$\frac{\lambda_j}{\sum_{i=1}^p\lambda_i}.$ Per tant, la variància explicada per les \red{$k$ primeres components} val: 
$\frac{\sum_{i=1}^k\lambda_j}{\sum_{i=1}^p\lambda_i}.$

\end{itemize}
}
\end{frame}


\begin{frame}
\frametitle{Propietats de l'ACP sobre la \red{matriu de covariàncies}}
{\small
\begin{itemize}
\item Sigui $\mathbf{X}_i$ la $i$-èssima \blue{variable original}. O sigui, la $i$-èssima columna de la matriu de 
dades~$\mathbf{X}$. Sigui $\mathbf{CP}_j$ la \red{$j$-èssima component principal}. O sigui, la $j$-èssima columna de la 
matriu de dades~$\mathbf{CP}$. Aleshores la covariància entre les variables (columnes) $\mathbf{X}_i$ i $\mathbf{CP}_j$ 
val:
\[
\mbox{cov}(\mathbf{X}_i,\mathbf{CP}_j)=\lambda_j u_{ji},
\]
on $u_{ji}$ és la $i$-èssima component del vector propi unitari $\mathbf{u}_j$ corresponent al valor propi~$\lambda_j.$ 

\item Seguin la mateixa notació anterior, la correlació entre $\mathbf{X}_i$ i $\mathbf{CP}_j$ val:
\[
\mbox{cor}(\mathbf{X}_i,\mathbf{CP}_j)=\frac{\sqrt{\lambda_j}u_{ji}}{s_i}.
\]
\end{itemize}
}
\end{frame}

\begin{frame}
\frametitle{Propietats de l'ACP sobre la \red{matriu de covariàncies}}
\begin{itemize}
\item En general si definim la matriu $\mathbf{S}_{X,CP}$ de components $s_{ij}=\mbox{cov}(\mathbf{X}_i,\mathbf{CP}_j)$, 
podem escriure:
\[
\mathbf{S}_{X,CP}= \mathbf{V}\mbox{diag}(\lambda_1,\ldots,\lambda_p),
\]
on $\mathbf{V}$ és la matriu de vectors propis de la matriu de covariàncies~$\mathbf{S}.$
\item En general si definim la matriu $\mathbf{R}_{X,CP}$ de components $r_{ij}=\mbox{cor}(\mathbf{X}_i,\mathbf{CP}_j)$, 
podem escriure:
\[
\mathbf{R}_{X,CP}= 
\mathbf{V}\mbox{diag}(\sqrt{\lambda_1},\ldots,\sqrt{\lambda_p})\mbox{diag}\left(\frac{1}{s_1},\ldots,\frac{1}{s_p}
\right),
\]
on $s_i$ és la desviació típica de la \blue{variable original} $\mathbf{X}_i$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Propietats de l'ACP sobre la \red{matriu de covariàncies}}
La primera \red{component principal} seria la varietat lineal de dimensió~$1$ (una recta) que conserva la major 
\red{variabilitat} (anomenada \red{inèrcia}) del ``núvol'' de punts.
\medskip

De la mateixa manera, les dues primeres \red{components principals} serien la varietat lineal de dimensió~$2$ (pla) que 
conserva la major \red{variabilitat} (anomenada \red{inèrcia}) del ``núvol'' de punts.
\medskip

En general, les $k$ primeres \red{components principals} serien la varietat lineal de dimensió~$k$ que conserva la major 
\red{variabilitat} (anomenada \red{inèrcia}) del ``núvol'' de punts.


\end{frame}


\begin{frame}
\frametitle{Exemple}
\begin{exemple}
Comprovem la relació anterior entre les \blue{variables originals} i les \red{components principals} en l'exemple dels 
infants.
Recordem que la matriu centrada de dades era:
\[
\tilde{\mathbf{X}}=\left(
<<echo=F,results=tex>>=
print(xtable(nens.cent,digits=1),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{Exemple}
\begin{exemple}
La matriu de \red{components principals} era:
\[
{\mathbf{CP}}=\left(
<<echo=F,results=tex>>=
print(xtable(CP.nens,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
\end{exemple}
\end{frame}


\begin{frame}
\frametitle{Exemple}
\begin{exemple}
La covariància entre les dues matrius anteriors val:
\[
\mbox{cov}(\tilde{X},{\mathbf{CP}})=\left(
<<echo=F,results=tex>>=
print(xtable(cov(nens.cent,CP.nens)*8/9,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]

\end{exemple}
\end{frame}


\begin{frame}
\frametitle{Exemple}
\begin{exemple}
Si fem $\mathbf{V}\mbox{diag}(\lambda_1,\ldots,\lambda_p)$ obtenim el mateix:
{
\begin{eqnarray*}
&&\left(
<<echo=F,results=tex>>=
print(xtable(veps.nens,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)\cdot
\\ && \hspace*{1cm}\left(
<<echo=F,results=tex>>=
print(xtable(diag(vaps.nens),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right) \\ && = \mbox{cov}(\tilde{X},{\mathbf{CP}})
\end{eqnarray*}

}

\end{exemple}
\end{frame}


\begin{frame}
\frametitle{Exemple}
\begin{exemple}
La correlacions entre les dues matrius anteriors val:
\[
\mbox{cor}(\tilde{X},{\mathbf{CP}})=\left(
<<echo=F,results=tex>>=
print(xtable(cor(nens.cent,CP.nens),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]

\end{exemple}
\end{frame}

\begin{frame}
\frametitle{Exemple}
\begin{exemple}
Si fem 
$\mathbf{V}\mbox{diag}(\sqrt{\lambda_1},\ldots,\sqrt{\lambda_p})\mbox{diag}\left(\frac{1}{s_1},\ldots,\frac{1}{s_p}
\right)$ obtenim el mateix, o sigui el producte de les tres matrius següents:
\begin{eqnarray*}
\mathbf{V} &=&\left(
<<echo=F,results=tex>>=
print(xtable(veps.nens,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)\\
\mbox{diag}(\sqrt{\lambda}) &=&\left(
<<echo=F,results=tex>>=
print(xtable(diag(sqrt(vaps.nens)),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\end{eqnarray*}


\end{exemple}
\end{frame}

\begin{frame}
\frametitle{Exemple}
\begin{exemple}
\[
\mbox{diag}(1/s)=\left(
<<echo=F,results=tex>>=
print(xtable(sqrt(diag(1/diag(cov.nens))),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]

\end{exemple}
\end{frame}

\begin{frame}
\frametitle{Exemple}
\begin{exemple}
Vegem quin percentatge de variabilitat tenim si consideram només les primeres \red{components principals}:
\begin{center}
\begin{tabular}{|l|l|}\hline
Variables&Varianza Explicada\\\hline
$\mathbf{CP}_1$&$\Sexpr{round(vaps.nens[1],3)}/\Sexpr{round(sum(vaps.nens),3)}=
\Sexpr{round(vaps.nens[1]/sum(vaps.nens),3)}$\\\hline
$\mathbf{CP}_{1,2}$&$\Sexpr{round(sum(vaps.nens[1:2]),3)}/\Sexpr{round(sum(vaps.nens),3)}=
\Sexpr{round(sum(vaps.nens[1:2])/sum(vaps.nens),3)}$\\\hline
$\mathbf{CP}_{1,2,3}$&$\Sexpr{round(sum(vaps.nens[1:3]),3)}/\Sexpr{round(sum(vaps.nens),3)}=
\Sexpr{round(sum(vaps.nens[1:3])/sum(vaps.nens),4)}$\\\hline
$\mathbf{CP}_{1,2,3,4}$&$1$\\\hline
\end{tabular}
\end{center}
En aquest exemple, si només tenguéssim en compte les dues \red{primeres components} explicaríem el 
$\Sexpr{round(sum(vaps.nens[1:2])*100/sum(vaps.nens),2)}\%$ de la \red{variabilitat} total.

\end{exemple}
\end{frame}

\section{ACP sobre la matriu de correlacions}
\begin{frame}
\frametitle{ACP sobre la \red{matriu de correlacions}}
Per realitzar l'ACP sobre la matriu de correlacions, es fa de la mateixa manera que l'ACP sobre la matriu de 
covariàncies però en lloc de fer servir aquesta matriu es fa servir la matriu de correlacions~$\mathbf{R}.$
\medskip

O sigui, es calculen els valors propis $\lambda_i$ de la matriu~$\mathbf{R}$ juntament amb la matriu de vectors 
propis~$\mathbf{V}$.
\medskip

Això és equivalent a aplicar l'ACP sobre la matriu de covariàncies però en lloc de fer servir la matriu centrada 
original, es fa servir la matriu de dades tipificada~$\mathbf{Z}.$
\medskip

Per tant, totes les propietats enunciades sobre la \red{matriu de covariàncies} serien vàlides per la \red{matriu de 
correlacions} substituint simplement la matriu $\mathbf{S}$ per la matriu $\mathbf{R}.$
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de correlacions}}
\begin{exemple}
Anem a repetir l'exemple dels infants però fent una ACP sobre la \red{matriu de correlacions.}

Recordem les dades:
{\small 
\begin{center}\begin{tabular}{|c|c|c|c|c|}\hline
$x_1$ & $x_2$ & $x_3$ & $x_4$&Sexe\\\hline
78&48.2&2.75&29.5&Nina\\ 69&45.5&2.15&26.3&Nina\\
77&46.3&4.41&32.2&Nina\\ 88&49&5.52&36.5&Nin\\ 67&43&3.21&27.2&Nina\\
80&48&4.32&27.7&Nina\\ 74&48&2.31&28.3&Nina\\ 94&53&4.3&30.3&Nin\\
102&58&3.71&28.7&Nin
\\\hline\end{tabular}\end{center}
}
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de correlacions}}
La matriu de dades tipificada serà:
<<echo=F>>=
aux=apply(nens,2,sd)
aux=aux*sqrt((n-1)/n)
Z=nens.cent%*%diag(1/aux)
@
\[
{\mathbf{R}}=\left(
<<echo=F,results=tex>>=
print(xtable(Z,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
\end{frame}



<<echo=F>>=
vaps.nens=eigen(cor(nens))$values
veps.nens=eigen(cor(nens))$vectors
@




\begin{frame}
\frametitle{ACP sobre la \red{matriu de correlacions}}
\begin{exemple}
La matriu de correlacions~$\mathbf{R}$ de la matriu $\mathbf{X}$ o la matriu de covariàncies de la matriu~$\mathbf{Z}$ 
serà:
\[
{\mathbf{R}}=\left(
<<echo=F,results=tex>>=
print(xtable(cor(nens),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
Els valors propis de la matriu de correlacions anterior són:
\[
<<echo=F,results=tex>>=
print(xtable(t(as.matrix(vaps.nens)),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\]

\end{exemple}
\end{frame}


\begin{frame}
\frametitle{ACP sobre la \red{matriu de correlacions}}
\begin{exemple}
Els corresponents vectors propis (per columnes) associats als vectors propis anteriors són:
\[
<<echo=F,results=tex>>=
print(xtable(veps.nens,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\]
Ens adonem que els vectors anteriors són ortogonals i estan normalitzats.


\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de correlacions}}
\begin{exemple}
{
Així la matriu que ens canviarà de \blue{variables originals} a \red{components principals} serà la transposada dels 
vectors propis:
\[
\Lambda=
\left(
<<echo=F,results=tex>>=
print(xtable(t(veps.nens),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
}
\end{exemple}
\end{frame}


\begin{frame}
\frametitle{ACP sobre la \red{matriu de correlacions}}
\begin{exemple}
Les expressions de les \red{components principals} en funció de les \blue{variables originals} tipificades són:
\begin{eqnarray*}
CP_1 &=& \Sexpr{signe0(veps.nens[1,1])}\Sexpr{round(abs(veps.nens[1,1]),3)}Z_1 
\Sexpr{signe(veps.nens[2,1])}\Sexpr{round(abs(veps.nens[2,1]),3)}Z_2
\Sexpr{signe(veps.nens[3,1])}\Sexpr{round(abs(veps.nens[3,1]),3)}Z_3 
\Sexpr{signe(veps.nens[4,1])}\Sexpr{round(abs(veps.nens[4,1]),3)}Z_4
\\
CP_2 &=& \Sexpr{signe0(veps.nens[1,2])}\Sexpr{round(abs(veps.nens[1,2]),3)}Z_1
\Sexpr{signe(veps.nens[2,2])}\Sexpr{round(abs(veps.nens[2,2]),3)}Z_2
\Sexpr{signe(veps.nens[3,2])}\Sexpr{round(abs(veps.nens[3,2]),3)}Z_3 
\Sexpr{signe(veps.nens[4,2])}\Sexpr{round(abs(veps.nens[4,2]),3)}Z_4
\\
CP_3 &=& \Sexpr{signe0(veps.nens[1,3])}\Sexpr{round(abs(veps.nens[1,3]),3)}Z_1
\Sexpr{signe(veps.nens[2,3])}\Sexpr{round(abs(veps.nens[2,3]),3)}Z_2
\Sexpr{signe(veps.nens[3,3])}\Sexpr{round(abs(veps.nens[3,3]),3)}Z_3 
\Sexpr{signe(veps.nens[4,3])}\Sexpr{round(abs(veps.nens[4,3]),3)}Z_4 \\
CP_4 &=& \Sexpr{signe0(veps.nens[1,4])}\Sexpr{round(abs(veps.nens[1,4]),3)}Z_1
\Sexpr{signe(veps.nens[2,4])}\Sexpr{round(abs(veps.nens[2,4]),3)}Z_2
\Sexpr{signe(veps.nens[3,4])}\Sexpr{round(abs(veps.nens[3,4]),3)}Z_3 
\Sexpr{signe(veps.nens[4,4])}\Sexpr{round(abs(veps.nens[4,4]),3)}Z_4
\end{eqnarray*}
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de correlacions}}
\begin{exemple}
{\small
Per tant, la matriu de dades en les noves variables serà:
<<echo=F>>=
CP.nens=Z%*%veps.nens
@
\[
\mathbf{Y}={\mathbf{Z}}\Lambda^\top = {\mathbf{Z}}\mathbf{V}=
\left(
<<echo=F,results=tex>>=
print(xtable(CP.nens,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
}
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de correlacions}}
\begin{exemple}
<<echo=F>>=
cp.nens.cor = cor(CP.nens)
@
Si calculam la matriu de correlacions de les \red{components principals} val:
\[
\mathbf{R}_{CP}=\left(
<<echo=F,results=tex>>=
print(xtable(cp.nens.cor,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
Surt la matriu diagonal, fet que posa de manifest que les \red{components principals} són incorrelades.
\end{exemple}
\end{frame}



\begin{frame}
\frametitle{ACP sobre la \red{matriu de correlacions}}
\begin{exemple}

La primera \red{component principal} (1a. columna de la matriu $\mathbf{CP}$) hereta el 
$\frac{\Sexpr{round(vaps.nens[1],3)}}{\Sexpr{round(sum(vaps.nens),3)}}\cdot 100\% = 
\Sexpr{round(vaps.nens[1]*100/sum(vaps.nens),3)}\%$ de la variabilitat total.

Les dues primeres \red{components principals} hereten el 
$\frac{\Sexpr{round(vaps.nens[1],3)} + \Sexpr{round(vaps.nens[2],3)}}{\Sexpr{round(sum(vaps.nens),3)}}\cdot 100\% = 
\Sexpr{round(sum(vaps.nens[1:2])*100/sum(vaps.nens),3)}\%$ de la variabilitat total.
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de correlacions}}
\begin{exemple}
Això ens fa pensar que si només tenim en compte les dues primeres components, podem fer un gràfic on hi estiguin 
representats tots els nens dibuixant de blau els nens i de vermell les nenes.
\medskip

Comprovem que les dues primeres components separen bé els nens i les nens. Concloem que la nostra taula de dades 
``explica'' la variable sexe.
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{ACP sobre la \red{matriu de correlacions}}
\begin{exemple}
\vspace*{-1cm}

<<echo=F,fig=T,width=5,height=5>>=
sexe=c("nina","nina","nina","nin","nina","nina","nina","nin","nin")
plot(CP.nens[,1],CP.nens[,2],xlab="CP1",ylab="CP2",col=ifelse(sexe=="nin","blue","red"))
@
\end{exemple}
\end{frame}

\section{Etapes d'un ACP}

\begin{frame}
\frametitle{Etapes d'un ACP}
\begin{itemize}
\item \blue{Primera etapa:} decidir si es realitza l'ACP damunt les dades brutes centrades (matriu de covariàncies) o 
sobre les dades tipificades (matriu de correlacions).
\begin{itemize}
\item Quan les variables originals $\mathbf{X}$ estan en unitats distintes, convé aplicar l'\red{ACP de correlacions}. 
Si estan en les mateixes unitats, ambdues alternatives són vàlides.
\item Si les diferències entre les variàncies són informatives i volem tenir-les en compte en l'anàlisi, no hem 
d'estandaritzar les variables i aplicar l'\red{ACP de covariàncies}.
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Etapes d'un ACP}
\begin{itemize}
\item \blue{Segona etapa:} reducció de la dimensionalitat. Hem de decidir quantes components retenim. La quantitat de 
variància retenguda serà:
\begin{center}
\begin{tabular}{|c|c|c|}\hline
Comp. & Valor propi & Quantitat retinguda\\\hline
$Cp_1$  & $\lambda_1$ & $\lambda_1/\sum_{i=1}^p \lambda_i$\\
$Cp_2 $ & $\lambda_2$ & $(\lambda_1+\lambda_2)/\sum_{i=1}^p \lambda_i$\\
$Cp_3$ & $\lambda_3$ &
$(\lambda_1+\lambda_2+\lambda_3)/\sum_{i=1}^p
\lambda_i$\\ $\ldots$ & $\ldots$ & $\ldots$\\
$Cp_p$ & $\lambda_p$ &
$(\lambda_1+\ldots+\lambda_p)/\sum_{i=1}^p \lambda_i=1$\\\hline
\end{tabular}
\end{center}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Etapes d'un ACP}
\begin{itemize}
\item \blue{Segona etapa:} Per decidir el  nombre de components retingudes, hi ha dos mètodes:
\begin{itemize}
\item Seleccionar components fins cobrir una proporció determinada de variància, com el $80\%$ o el $90\%$. 
\item Mètode de la mitjana aritmètica. Se retenen totes aquelles components $\mathbf{CP}_i$ que compleixin que 
$\lambda_i \geq \overline{\lambda}=\frac{\sum_{i=1}^p \lambda_i}{p}$. En el cas de l'\red{ACP de correlacions}, la 
condició anterior és $\lambda_i\geq 1$.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Etapes d'un ACP}
\begin{exemple}
En l'exemple dels infants,
\begin{itemize}
\item si aplicam el primer mètode per decidir el nombre de components retingudes, fent l'\red{ACP de correlacions}, si 
només elegim la primera component, ja cobrim el $\Sexpr{round(vaps.nens[1]*100/sum(vaps.nens),3)}\%$ de la variància 
total. Si elegim les dues primeres, cobrim el $\Sexpr{round(sum(vaps.nens[1:2])*100/sum(vaps.nens),3)}\%$ de la 
variància total.
\item si aplicam el mètode de la mitjana aritmètica, hauríem de retenir dues components ja que  els valors propis de la 
matriu de correlacions~$\mathbf{R}$ eren 
$
<<echo=F,results=tex>>=
print(xtable(as.matrix(t(vaps.nens)),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
$.
\end{itemize}
\end{exemple}
\end{frame}


\section{Descomposició en valors singulars (SVD)}
\begin{frame}
\frametitle{Descomposició en valors singulars}
Donada una matriu de dades $\mathbf{X}$ de dimensions $n\times p$, on $n\geq p$ i de rang~$p$, es pot descompondre en 
producte de tres matrius:
\[
\mathbf{X}=\mathbf{U}\Sigma\mathbf{V}^\top,
\]
on
\begin{itemize}
\item $\mathbf{U}$ és una matriu ortogonal $n\times p$ que té per columnes els $p$ vectors propis de la matriu 
$\mathbf{X}\mathbf{X}^\top$ associats als $p$ valors propis no nuls.
\item ${\Sigma}$ és una matriu diagonal $p\times p$ que té per diagonal les arrels quadrades dels valors propis de la 
matriu $\mathbf{X}^\top\mathbf{X}$.
\item $\mathbf{V}$ és una matriu ortogonal $p\times p$ que té per columnes els vectors propis de la matriu 
$\mathbf{X}^\top\mathbf{X}$ associats als $p$ valors propis no nuls.
\end{itemize}
\end{frame}

<<echo=F>>=
XtX=t(nens.cent)%*%nens.cent
XXt=nens.cent%*%t(nens.cent)
l2=eigen(XtX)$values
m.u=eigen(XXt)$vectors[,1:4]
m.v=eigen(XtX)$vectors
m.Sigma=diag(sqrt(l2))
@

\begin{frame}
\frametitle{Descomposició en valors singulars (SVD)}
\begin{exemple}
Considerem la matriu $\mathbf{X}$ com la matriu de dades centrada de l'exemple dels infants.
\medskip

La matriu $\mathbf{X}^\top\mathbf{X}$ val:
\[
\mathbf{X}^\top\mathbf{X}=\left(
<<echo=F,results=tex>>=
print(xtable(XtX,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{Descomposició en valors singulars (SVD)}
\begin{exemple}
La matriu $\mathbf{X}\mathbf{X}^\top$ val: (mostram només les 4 primeres columnes, pensau que és $10\times 10$)
{\footnotesize
\[
\mathbf{X}^\top\mathbf{X}=\left(
<<echo=F,results=tex>>=
print(xtable(XXt[,1:4],digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
}
\end{exemple}
\end{frame}


\begin{frame}
\frametitle{Descomposició en valors singulars  (SVD)}
\begin{exemple}
Els valors propis de la matriu $\mathbf{X}^\top\mathbf{X}$ són:
\[
<<echo=F,results=tex>>=
print(xtable(as.matrix(t(l2)),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\]
Per tant, la matriu $\Sigma$ serà:
\[
\Sigma =\left(
<<echo=F,results=tex>>=
print(xtable(m.Sigma,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
\end{exemple}
\end{frame}


\begin{frame}
\frametitle{Descomposició en valors singulars (SVD)}
\begin{exemple}
La matriu $\mathbf{U}$ serà la següent matriu $10\times 4$:
\[
\mathbf{U} =\left(
<<echo=F,results=tex>>=
print(xtable(m.u,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
\end{exemple}
\end{frame}


\begin{frame}
\frametitle{Descomposició en valors singulars (SVD)}
\begin{exemple}
La matriu $\mathbf{V}$ serà la següent matriu $4\times 4$:
\[
\mathbf{U} =\left(
<<echo=F,results=tex>>=
print(xtable(m.v,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]

Es pot comprovar que $\mathbf{X}=\mathbf{U}\Sigma\mathbf{V}^\top$.
\end{exemple}
\end{frame}

\section{Relació  ACP amb SVD}

\begin{frame}
\frametitle{Relació ACP amb SVD}
Considerem una matriu de dades $\mathbf{X}$ $n\times p$ que pot ésser centrada (\red{ACP de covariàncies}) o tipificada 
(\red{ACP de correlacions}).
\medskip

Si considerem la seva \red{SVD}, $\mathbf{X}=\mathbf{U}\Sigma\mathbf{V}^\top$, tenim que les \red{components 
principals}, $\mathbf{Y}$, valen $\mathbf{CP}=\mathbf{U}\Sigma$.
\medskip

{\footnotesize
La prova és molt senzilla. Recordem que les components principals valien: $\mathbf{CP}=\mathbf{X}\mathbf{V}$, on 
$\mathbf{V}$ era la matriu de vectors propis de la matriu de covariàncies 
$\mathbf{S}=\frac{1}{n}\mathbf{X}^\top\mathbf{X}$. Ara bé, aquesta matriu coincidirà amb la matriu de vectors propis de 
la matriu $\mathbf{X}^\top\mathbf{X}$ ja que els vectors propis de la matriu anterior i de la matriu de covariàncies 
$\mathbf{S}$ són els mateixos.

Per tant, 
$$\mathbf{Y}=\mathbf{X}\mathbf{V}=\mathbf{U}\Sigma\mathbf{V}^\top\mathbf{V}=\mathbf{U}\Sigma,$$
ja que la matriu $\mathbf{V}$ és ortogonal.}
\end{frame}

\begin{frame}
\frametitle{Relació ACP amb SVD}
\begin{teorema}
El producte escalar de dues \blue{files} de la matriu de dades $\mathbf{X}$ coincideix amb el producte escalar de dues 
\blue{files} de la matriu de  \red{components principals} $\mathbf{Y}$.
\end{teorema}
{\footnotesize
Prova. El producte escalar de dues files de la matriu~$\mathbf{X}$ ve donada per la matriu $\mathbf{X}\mathbf{X}^\top$ 
però:
\[
\mathbf{X}\mathbf{X}^\top = \mathbf{Y}\mathbf{V}^\top\mathbf{V}\mathbf{Y}^\top =\mathbf{Y}\mathbf{Y}^\top,
\]
aquesta última matriu ens dóna el producte escalar de dues files de la matriu de \red{components principals.}
}
\end{frame}

\begin{frame}
\frametitle{Relació ACP amb SVD}
\begin{teorema}
La distància euclídea de dues files de la matriu $\mathbf{X}$ coincideix amb la distància euclídea de la matriu de 
\red{components principals} $\mathbf{Y}$. O sigui, la distància entre els individus respecte les \blue{variables 
originals} i respecte les \red{components principals} se conserva.
\end{teorema}
{\footnotesize Prova. Siguin $\mathbf{f}_i$ i $\mathbf{f}_j$ dues files de la matriu $\mathbf{X}$ i siguin 
$\mathbf{g}_i$ i $\mathbf{g}_j$ dues files de la matriu $\mathbf{Y}$. Aleshores:
\[
\|\mathbf{f}_i-\mathbf{f}_j\|^2 = (\mathbf{f}_i-\mathbf{f}_j)^\top 
(\mathbf{f}_i-\mathbf{f}_j)=\mathbf{f}_i^\top\mathbf{f}_i-2\mathbf{f}_i^\top\mathbf{f}_j+\mathbf{f}_j^\top\mathbf{f}_j.
\]
Ara bé, fent servir el teorema anterior tenim que:
\[
\mathbf{f}_i^\top\mathbf{f}_i=\mathbf{g}_i^\top\mathbf{g}_i,\ 
\mathbf{f}_i^\top\mathbf{f}_j=\mathbf{g}_i^\top\mathbf{g}_j, \ 
\mathbf{f}_j^\top\mathbf{f}_j=\mathbf{g}_j^\top\mathbf{g}_j.
\]
Concloem, doncs:
\(
\|\mathbf{f}_i-\mathbf{f}_j\|^2=\|\mathbf{g}_i-\mathbf{g}_j\|^2.
\)
}
\end{frame}


\begin{frame}
\frametitle{Relació ACP amb SVD}
\begin{teorema}
Sigui $\mathbf{X}$ la nostra matriu de dades originals centrada o tipificada. Sigui 
$\mathbf{X}=\mathbf{U}\Sigma\mathbf{V}^\top$ la seva \red{SVD}. Considerem la matriu $\mathbf{V}_2 = 
\Sigma\mathbf{V}^\top$. Aleshores el producte escalar de dues \blue{columnes} de la matriu $\mathbf{X}$ i de dues 
\blue{columnes} de la matriu $\mathbf{V}_2$ és el mateix.
\end{teorema}
{\footnotesize Prova.
El producte escalar de dues columnes de la matriu $\mathbf{X}$ ho dóna la matriu $\mathbf{X}^\top\mathbf{X}$. Tendrem:
\[
\mathbf{X}^\top\mathbf{X}=\mathbf{V}\Sigma\mathbf{U}^\top\mathbf{U}\Sigma\mathbf{V}^\top = \mathbf{V}\Sigma 
(\mathbf{V}\Sigma)^\top = \mathbf{V}_2^\top\mathbf{V}_2,
\]
matriu que dóna el producte escalar de dues columnes de la matriu~$\mathbf{V}_2$.
}
\end{frame}


\begin{frame}
\frametitle{Relació ACP amb SVD}
\begin{teorema}
Sigui $\mathbf{X}$ la nostra matriu de dades originals centrada o tipificada. Sigui 
$\mathbf{X}=\mathbf{U}\Sigma\mathbf{V}^\top$ la seva \red{SVD}. Considerem la matriu $\mathbf{V}_2 = 
\Sigma\mathbf{V}^\top$. Aleshores la correlació entre dues columnes de la matriu $\mathbf{X}$ i la correlació entre dues 
columnes de la matriu $\mathbf{V}_2 = \Sigma\mathbf{V}^\top$ és la mateixa.
O sigui, la correlació entre dues \blue{variables originals} (dues columnes de la matriu~$\mathbf{X}$) i dues columnes 
de la matriu $\mathbf{V}_2$ és la mateixa.
\end{teorema}

Per la prova, basta aplicar el teorema anterior. 
\end{frame}

\section{Biplots}
\begin{frame}
\frametitle{Biplots}
Per representar el resultat d'un ACP gràficament es fa servir el que s'anomena un \red{biplot}. 
\medskip

Un \red{biplot} és un gràfic bidimensional on es representa en el mateix gràfic els individus i les \blue{variables 
originals.}
\medskip

El \red{biplot} només té significat si la variabilitat explicada per les dues primeres \red{components principals} és 
alta, posem d'un $85\%$ o més. 
\medskip

En un \red{biplot} hi ha $4$ eixos coordenats: dos fan referència als individus (eixos d'abaix i de l'esquerra) i dos 
fan referència a les \blue{variables originals.} (eixos de dalt i de la dreta)
\end{frame}

\begin{frame}
\frametitle{Biplots}
Els eixos que fan referència als individus són els corresponents a les dues \red{components principals}.
\medskip

Per tant, les coordenades dels individus seran els valors de les dues \red{primeres components} estandaritzades (ja 
veurem què significa això amb un exemple).
\medskip

Hem vist que la distància euclídea entre els valors originals dels individus (files de la matriu de dades~$\mathbf{X}$) 
i els valors d'aquests mateixos individus respecte les \red{components principals} se conserva. Això significa que si 
les dues primeres \red{components principals} expliquen molta variabilitat, la distància entre individus que es veurà al 
\red{biplot} serà aproximadament la distància entre els individus segons les \blue{variables originals.}
\medskip

\end{frame}


\begin{frame}
\frametitle{Biplots}

Les coordenades corresponents a les \blue{variables originals} (columnes de la matriu de dades~$\mathbf{X}$) les ens 
dóna les dues primeres components en columnes de la matriu $\mathbf{V}_2 = \Sigma\mathbf{V}^\top$ ja que hem vist que la 
correlació entre les \blue{variables originals} i les columnes de la matriu anterior és la mateixa.

\end{frame}

\begin{frame}
\frametitle{Biplots}
El gràfic de les \blue{variables originals} es va mitjançant vectors. La interpretació que s'ha de fer és la següent: si 
l'angle entre dues \blue{variables originals} és petit, significa que el cosinus d'aquest angle serà gran però aquest 
cosinus és la correlació entre les dues variables. Per tant, hi haurà molta correlació entre les variables. En canvi, si 
l'angle entre les dues variables està proper a un angle recte, la correlació entre aquestes variables és quasi nul·la.
\medskip

Les llargades dels vectors són les variàncies de les \blue{variables originals}. Per tant, com més llarg tengui un 
vector, la variable corresponent tendrà més dispersió.
\end{frame}


<<echo=F>>=
n=9
H9=matrix(-1/n,n,n)
diag(H9)=rep(1-1/n,n)
nens.cent=H9%*%nens
cov.nens=(1/n)*t(nens.cent)%*%nens.cent
vaps.nens=eigen(cov.nens)$values
veps.nens=eigen(cov.nens)$vectors
CP.nens=nens.cent%*%veps.nens
sigma=diag(svd(nens.cent)$d)
m.v2=sigma%*%t(veps.nens)
estudi.acp=prcomp(nens.cent)
@

\begin{frame}
\frametitle{Biplots}
\begin{exemple}
Considerem l'exemple dels infants fent l'\red{ACP de covariàncies}.

Recordem que les dues primeres \red{components principals} explicaven el 
$\Sexpr{round(sum(vaps.nens[1:2])*100/sum(vaps.nens),3)}\%$ de la variabilitat total.

Les \red{components principals} eren les següents:
{\footnotesize
\[
\mathbf{Y}=
\left(
<<echo=F,results=tex>>=
print(xtable(CP.nens,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
}
\end{exemple}
\end{frame}


\begin{frame}
\frametitle{Biplots}
\begin{exemple}
Quan representem els individus (nens i nenes) en el \red{biplot} representarem les dues primeres columnes de la matriu 
anterior tipificada. 

Això significa que dividirem cada element de la matriu per la norma euclídea de la columna. Així la norma euclídea de la 
primera columna de la matriu anterior val:
\[
\sqrt{\Sexpr{round(abs(CP.nens[1,1]),3)}^2+\cdots+\Sexpr{round(abs(CP.nens[9,1]),3)}^2}=
\Sexpr{round(sqrt(sum(CP.nens[,1]^2)),3)}.
\]
Fent el mateix amb la segona columna,
\[
\sqrt{\Sexpr{round(abs(CP.nens[1,2]),3)}^2+\cdots+\Sexpr{round(abs(CP.nens[9,2]),3)}^2}=
\Sexpr{round(sqrt(sum(CP.nens[,2]^2)),3)}.
\]
\end{exemple}
\end{frame}


\begin{frame}
\frametitle{Biplots}
\begin{exemple}
Les coordenades dels nens i nenes en el biplot seran:
\[
\left(
<<echo=F,results=tex>>=
print(xtable(CP.nens[,1:2],digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\left(
\begin{array}{cc}
\frac{1}{\Sexpr{round(sqrt(sum(CP.nens[,1]^2)),3)}}&0\\
0&\frac{1}{\Sexpr{round(sqrt(sum(CP.nens[,2]^2)),3)}}
\end{array}
\right)=
\]
\end{exemple}
\end{frame}

<<echo=F>>=
coor.nens.biplot = CP.nens[,1:2]%*%diag(c(1/sqrt(sum(CP.nens[,1]^2)), 
1/sqrt(sum(CP.nens[,2]^2))))
@

\begin{frame}
\frametitle{Biplots}
\begin{exemple}
Les coordenades dels nens i nenes en el biplot seran:
\[
\left(
<<echo=F,results=tex>>=
print(xtable(coor.nens.biplot,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
\end{exemple}
\end{frame}


\begin{frame}
\frametitle{Biplots}
\begin{exemple}
Passem ara a calcular les coordenades de les \blue{variables originals}.
\medskip

Recordem que la matriu de vectors propis de la matriu de covariàncies~$\mathbf{S}$ era:
\[
\left(
<<echo=F,results=tex>>=
print(xtable(veps.nens,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{Biplots}
\begin{exemple}
Si fem la \red{SVD} de la matriu de dades centrada $\tilde{\mathbf{X}}$, la matriu $\Sigma$ serà:
\[
\Sigma=\left(
<<echo=F,results=tex>>=
print(xtable(sigma,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
Per tant, la matriu $\mathbf{V}_2 =\Sigma\mathbf{V}^\top$ serà:
\[
\mathbf{V}_2 = \left(
<<echo=F,results=tex>>=
print(xtable(m.v2,digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{Biplots}
\begin{exemple}
Per tant, les coordenades de les $4$ variables originals són:
\begin{itemize}
\item[${\mathbf{x}_1}$]: Edat en dies. $\left(
<<echo=F,results=tex>>=
print(xtable(as.matrix(t(m.v2[1:2,1])),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
$

\item[${\mathbf{x}_2}$]: Alçada en néixer. $\left(
<<echo=F,results=tex>>=
print(xtable(as.matrix(t(m.v2[1:2,2])),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
$

\item[${\mathbf{x}_3}$]: Pes en néixer $\left(
<<echo=F,results=tex>>=
print(xtable(as.matrix(t(m.v2[1:2,3])),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
$

\item[${\mathbf{x}_4}$]: Augment en tant per cent del seu pes actual respecte del seu pes en néixer $\left(
<<echo=F,results=tex>>=
print(xtable(as.matrix(t(m.v2[1:2,4])),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
$
\end{itemize}
\end{exemple}
\end{frame}

\begin{frame}
\frametitle{Biplots}
A continuació mostram el biplot:
\vspace*{0cm}

<<fig=TRUE,fig.align="center",width=5,height=5,echo=FALSE>>=
biplot(estudi.acp)
@
\end{frame}

\begin{frame}
\frametitle{Biplots}
Comprovem les coordenades dels punts i de les variables.
\medskip

Veiem també que entre les parelles de variables $(\mathbf{x}_1, \mathbf{x}_2)$ i   $(\mathbf{x}_3,\mathbf{x}_4)$ hi ha 
bastanta correlació i entre les parelles $(\mathbf{x}_2, \mathbf{x}_3)$ i $(\mathbf{x}_2, \mathbf{x}_4)$ n'hi ha poca 
com podem comprovar si calculem la correlació entre les variables originals:
\[
\left(
<<echo=F,results=tex>>=
print(xtable(cor(nens.cent),digits=3),type="latex",floating=F,
include.rownames=F,
include.colnames=F,hline.after=NULL)
@
\right)
\]
Observem que les variables amb més dispersió són l'edat de l'infant i l'alçada en néixer.
\end{frame}

\end{document}



